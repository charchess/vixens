# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

Vixens is a multi-cluster Kubernetes homelab infrastructure following GitOps best practices. The project is built in phases: **Phase 1** provisions infrastructure with Terraform, **Phase 2** deploys services via ArgoCD, and **Phase 3** runs applications.

**Core Stack:**
- **OS**: Talos Linux v1.11.0 (immutable, API-driven)
- **Kubernetes**: v1.34.0
- **Infrastructure**: Terraform + Talos provider
- **GitOps**: ArgoCD v7.7.7 (App-of-Apps pattern) - âœ… Deployed
- **CNI**: Cilium v1.18.3 (eBPF, kube-proxy replacement, Hubble observability, L2 Announcements)
- **LoadBalancer**: Cilium L2 Announcements + LB IPAM - âœ… Deployed
- **Ingress**: Traefik v3.x - Phase 2
- **Storage**: Synology CSI (iSCSI) - Phase 2

## Current Phase: Phase 2 (GitOps Infrastructure)

**Status**: Sprint 4 COMPLETED - Full GitOps automation with zero manual kubectl commands âœ…

The project is iterative with a **destroy/recreate** strategy for dev/test environments to ensure reproducibility.

## Architecture

### Multi-Cluster Strategy

The infrastructure consists of 4 independent clusters, each on a dedicated VLAN:

| Environment | Nodes                     | VLAN Internal | VLAN Services | VIP              | Status |
|-------------|---------------------------|---------------|---------------|------------------|--------|
| **Dev**     | obsy, onyx, opale (3 CP HA) | 111         | 208           | 192.168.111.160  | âœ… Active |
| **Test**    | carny, celesty, citrine   | 111           | 209           | 192.168.111.180  | â³ Sprint 9 |
| **Staging** | TBD (3 nodes)             | 111           | 210           | 192.168.111.190  | ğŸ“… Future |
| **Prod**    | Physical nodes (3)        | 111           | 201           | 192.168.111.200  | ğŸ“… Phase 3 |

### Dual-VLAN Network Architecture

Each node has **two VLANs** configured on a single physical interface:

- **VLAN 111** (192.168.111.0/24) - **Non-routed, internal**
  - Inter-node communication (etcd, kubelet, CNI)
  - Storage access (Synology NAS: 192.168.111.69)
  - Kubernetes API VIP
  - Management host: grenat (192.168.111.64)

- **VLAN 20X** (192.168.20X.0/24) - **Routed, services**
  - External service exposure (Ingress, LoadBalancer)
  - Cilium LB IPAM pools: .70-.79 (assigned), .80-.89 (auto)
  - Internet gateway configured on this VLAN

### Repository Structure

```
vixens/
â”œâ”€â”€ terraform/                      # Phase 1: Infrastructure as Code
â”‚   â”œâ”€â”€ modules/talos/             # Reusable Talos cluster module âœ…
â”‚   â”‚   â”œâ”€â”€ main.tf                # Resources + per-node patches
â”‚   â”‚   â”œâ”€â”€ variables.tf           # Per-node config (disk, network, etc.)
â”‚   â”‚   â”œâ”€â”€ outputs.tf             # kubeconfig, talosconfig
â”‚   â”‚   â”œâ”€â”€ providers.tf           # Provider documentation
â”‚   â”‚   â””â”€â”€ versions.tf            # Terraform >= 1.5.0, Talos ~> 0.9
â”‚   â””â”€â”€ environments/
â”‚       â”œâ”€â”€ dev/                   # Dev cluster (obsy, onyx, opale - 3 CP HA) âœ…
â”‚       â”‚   â”œâ”€â”€ main.tf            # Module call with node configs
â”‚       â”‚   â”œâ”€â”€ versions.tf        # Provider versions
â”‚       â”‚   â”œâ”€â”€ provider.tf        # Provider config
â”‚       â”‚   â”œâ”€â”€ kubeconfig-dev     # Generated (gitignored)
â”‚       â”‚   â””â”€â”€ talosconfig-dev    # Generated (gitignored)
â”‚       â”œâ”€â”€ test/                  # Test cluster â³ Sprint 9
â”‚       â”œâ”€â”€ staging/               # Staging cluster ğŸ“… Future
â”‚       â””â”€â”€ prod/                  # Prod cluster ğŸ“… Future
â”‚
â”œâ”€â”€ argocd/                        # Phase 2: ArgoCD self-management
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ argocd-install.yaml   # ArgoCD Helm Application
â”‚   â”‚   â””â”€â”€ root-app.yaml         # App-of-Apps root
â”‚   â””â”€â”€ overlays/
â”‚       â”œâ”€â”€ dev/
â”‚       â”œâ”€â”€ test/
â”‚       â”œâ”€â”€ staging/
â”‚       â””â”€â”€ prod/
â”‚
â”œâ”€â”€ apps/                          # Phase 2: Infrastructure apps
â”‚   â”œâ”€â”€ cilium-lb/                 # âœ… Cilium L2 Announcements + LB IPAM
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”‚   â””â”€â”€ ippool.yaml
â”‚   â”‚   â””â”€â”€ overlays/
â”‚   â”‚       â””â”€â”€ dev/               # VLAN 208 pools (192.168.208.70-89)
â”‚   â”œâ”€â”€ traefik/
â”‚   â”œâ”€â”€ cert-manager/
â”‚   â”œâ”€â”€ synology-csi/
â”‚   â”œâ”€â”€ authelia/
â”‚   â””â”€â”€ monitoring/
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ network-diagram.md
â”‚   â”‚   â”œâ”€â”€ storage-strategy.md
â”‚   â”‚   â””â”€â”€ gitops-workflow.md
â”‚   â”œâ”€â”€ adr/                       # Architecture Decision Records
â”‚   â”‚   â”œâ”€â”€ 001-talos-linux.md
â”‚   â”‚   â”œâ”€â”€ 002-argocd-gitops.md
â”‚   â”‚   â”œâ”€â”€ 003-vlan-segmentation.md
â”‚   â”‚   â”œâ”€â”€ 004-cilium-cni.md
â”‚   â”‚   â””â”€â”€ 005-cilium-l2-announcements.md
â”‚   â””â”€â”€ ROADMAP.md                 # Sprint-based roadmap
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ validate-terraform.yaml
â”‚   â”œâ”€â”€ validate-kustomize.yaml
â”‚   â””â”€â”€ promote.yaml
â”‚
â”œâ”€â”€ CLAUDE.md                      # This file
â””â”€â”€ README.md                      # Quick start guide
```

## Development Commands

### Terraform (Phase 1)

```bash
# Working directory for dev environment
cd /root/vixens/terraform/environments/dev

# Format all Terraform files
terraform fmt -recursive

# Initialize Terraform
terraform init

# Validate configuration
terraform validate

# Plan infrastructure changes
terraform plan

# Apply infrastructure changes (creates cluster)
terraform apply

# Check state
terraform show

# DESTROY infrastructure (see lifecycle section below)
terraform destroy
```

### Terraform Destroy/Recreate Lifecycle

The project follows an **iterative destroy/recreate strategy** for dev/test environments:

**When to destroy:**
- After completing a major sprint to validate reproducibility
- Before significant refactoring
- To test clean slate provisioning
- NOT during normal development (just apply changes)

**Destroy/Recreate workflow:**
```bash
cd terraform/environments/dev

# 1. Destroy cluster
terraform destroy -auto-approve

# 2. Recreate from scratch
terraform apply -auto-approve

# 3. Validate cluster is functional
talosctl --talosconfig=talosconfig-dev --nodes 192.168.111.162 --endpoints 192.168.111.162 version
kubectl --kubeconfig=kubeconfig-dev get nodes
```

**Important notes:**
- Destroy is **safe** for dev/test (virtualized nodes)
- Destroy is **dangerous** for prod (physical infrastructure)
- Always commit Terraform code before destroying
- Validation: `terraform plan` should show "no changes" after recreate

### Talos Node Management

```bash
# Set config files as env vars (recommended)
export KUBECONFIG=/root/vixens/terraform/environments/dev/kubeconfig-dev
export TALOSCONFIG=/root/vixens/terraform/environments/dev/talosconfig-dev

# Check Talos version
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 version

# Check node health
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 health

# Check services status
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 services

# Check etcd members (cluster)
talosctl --nodes 192.168.111.160 etcd members

# Check network configuration
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 get addresses
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 get links

# View machine configuration
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 get machineconfig -o yaml

# Reboot node
talosctl --nodes 192.168.111.162 --endpoints 192.168.111.162 reboot
```

### Kubernetes (via kubectl)

```bash
# Check nodes
kubectl get nodes -o wide

# Check system pods
kubectl get pods -n kube-system

# Check all resources
kubectl get all -A

# View node details
kubectl describe node obsy

# Access logs
kubectl logs -n kube-system <pod-name>
```

### Validation & Testing

```bash
# Terraform validation
cd terraform/environments/dev
terraform validate
terraform fmt -check -recursive

# Check Cilium status (after Phase 2)
cilium status
cilium connectivity test

# Network connectivity tests
ping 192.168.111.162  # VLAN 111 (internal)
ping 192.168.208.162  # VLAN 208 (services)
```

## Terraform Module: talos

Location: `terraform/modules/talos/`

The Talos module provisions Kubernetes control plane clusters on Talos Linux with **per-node configuration**.

### Key Features âœ…

- **Per-node configuration**: Each node has its own disk, network, and patches
- **Dual-VLAN support**: Automatic configuration of internal (111) + services (20X) VLANs
- **VIP automatic configuration**: VIP extracted from cluster_endpoint and configured on VLAN 111
- **Hostname configuration**: Automatic hostname setup for all nodes (control plane + workers)
- **certSANs with VIP**: API server certificates automatically include VIP
- **Worker node support**: Support for both control plane and worker nodes
- **Automatic node reset on destroy**: talosctl reset provisioner cleans nodes on destroy
- **Odd control plane validation**: Enforces etcd quorum requirements (1, 3, 5 nodes)
- **Custom image support**: Optional factory/schematic images for extensions (iSCSI, etc.)
- **Automatic bootstrap**: First node is bootstrapped automatically
- **Config file generation**: Creates kubeconfig and talosconfig locally
- **Destroy/recreate safe**: Fully tested and reproducible infrastructure

### Module Interface

**Required Variables:**

| Variable | Type | Description | Example |
|----------|------|-------------|---------|
| `cluster_name` | string | Cluster name | `"vixens-dev"` |
| `cluster_endpoint` | string | Kubernetes API VIP | `"https://192.168.111.160:6443"` |
| `control_plane_nodes` | map(object) | **Per-node config** (see below) | `{ "obsy" = {...} }` |

**control_plane_nodes object structure:**
```hcl
{
  name         = string           # Node hostname
  ip_address   = string           # IP for Talos API access (initially external VLAN)
  mac_address  = string           # MAC address
  install_disk = string           # Install disk path (e.g., "/dev/sda")
  network = object({
    interface = string            # Physical interface name
    vlans = list(object({
      vlanId    = number          # VLAN ID (111, 208, etc.)
      addresses = list(string)    # IP addresses with CIDR
      gateway   = string          # Gateway (empty for non-routed VLANs)
    }))
  })
}
```

**Optional Variables:**

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `talos_version` | string | `"v1.11.3"` | Talos Linux version |
| `kubernetes_version` | string | `"v1.30.0"` | Kubernetes version |
| `talos_image` | string | `""` | Custom factory image URL for extensions |
| `pod_subnet` | string | `"10.244.0.0/16"` | Pod CIDR |
| `service_subnet` | string | `"10.96.0.0/12"` | Service CIDR |

**Outputs:**

| Output | Description | Sensitive |
|--------|-------------|-----------|
| `kubeconfig` | Kubernetes config file content | Yes |
| `talosconfig` | Talos config file content | Yes |
| `control_plane_configs` | Per-node machine configs | Yes |
| `cluster_endpoint` | Kubernetes API endpoint | No |

### Example Usage

```hcl
module "talos_cluster" {
  source = "../../modules/talos"

  cluster_name     = "vixens-dev"
  talos_version    = "v1.11.3"
  cluster_endpoint = "https://192.168.111.160:6443"

  # Optional: Custom image with iSCSI extension
  # talos_image = "factory.talos.dev/installer/<schematic_id>:v1.11.3"

  control_plane_nodes = {
    "obsy" = {
      name         = "obsy"
      ip_address   = "192.168.0.162"  # Initial maintenance IP for Terraform access
      mac_address  = "00:15:5D:00:CB:10"
      install_disk = "/dev/sda"
      network = {
        interface = "enx00155d00cb10"
        vlans = [
          {
            vlanId    = 111
            addresses = ["192.168.111.162/24"]
            gateway   = ""
          },
          {
            vlanId    = 208
            addresses = ["192.168.208.162/24"]
            gateway   = "192.168.208.1"
          }
        ]
      }
    }
  }

  worker_nodes = {
    "onyx" = {
      name         = "onyx"
      ip_address   = "192.168.0.164"  # Initial maintenance IP for Terraform access
      mac_address  = "00:15:5D:00:CB:11"
      install_disk = "/dev/sda"
      network = {
        interface = "enx00155d00cb11"
        vlans = [
          {
            vlanId    = 111
            addresses = ["192.168.111.164/24"]
            gateway   = ""
          },
          {
            vlanId    = 208
            addresses = ["192.168.208.164/24"]
            gateway   = "192.168.208.1"
          }
        ]
      }
    }
  }
}

# Generate config files locally
resource "local_file" "kubeconfig" {
  content         = module.talos_cluster.kubeconfig
  filename        = "${path.module}/kubeconfig-dev"
  file_permission = "0600"
}
```

## Current Infrastructure Status (Sprints 1-4 COMPLETED âœ…)

### Dev Cluster âœ…

| Component | Status | Details |
|-----------|--------|---------|
| **Terraform module** | âœ… Done | VIP, hostname, certSANs, destroy/recreate validated |
| **Node obsy** | âœ… Deployed | Control plane - 192.168.111.162 + VIP 192.168.111.160 |
| **Node onyx** | âœ… Deployed | Control plane - 192.168.111.164 |
| **Node opale** | âœ… Deployed | Control plane - 192.168.111.163 |
| **Talos** | âœ… Running | v1.11.0, Kubernetes v1.34.0 |
| **VIP HA** | âœ… Active | 192.168.111.160/32 on VLAN 111 |
| **etcd Quorum** | âœ… Active | 3 members HA |
| **Cilium CNI** | âœ… Running | v1.18.3 with L2 Announcements + LB IPAM |
| **ArgoCD** | âœ… Running | v7.7.7 with root-app auto-bootstrapped |
| **Cilium LB** | âœ… Active | Pool 192.168.208.70-89 (20 IPs available) |
| **VLANs** | âœ… Configured | VLAN 111 (internal) + VLAN 208 (services) |
| **Config files** | âœ… Generated | kubeconfig-dev, talosconfig-dev (local) |
| **GitOps Automation** | âœ… Complete | Zero manual kubectl commands |
| **Destroy/Recreate** | âœ… Validated | Fully reproducible infrastructure |

### Completed Sprints

| Sprint | Component | Status |
|--------|-----------|--------|
| **1** | **Terraform module Talos + dev 1 node** | **âœ… DONE** |
| **2** | **Cilium CNI v1.18.3** | **âœ… DONE** |
| **3** | **Scale to 3 control planes HA** | **âœ… DONE** |
| **4** | **ArgoCD bootstrap + full automation** | **âœ… DONE** |
| **5 (partial)** | **Cilium L2 Announcements (MetalLB replacement)** | **âœ… DONE** |

### Next Sprints

| Sprint | Component | Status |
|--------|-----------|--------|
| 5 | Traefik Ingress | ğŸ“… Next |
| 6 | cert-manager (TLS) | ğŸ“… Future |
| 7 | Synology CSI | ğŸ“… Future |
| 8-11 | Phase 2 services | ğŸ“… Future |

## Important Notes

### Phase 1 (Completed âœ…)
- **Terraform-managed**: All infrastructure is code
- **Immutable**: Talos nodes are disposable and reproducible
- **Destroy/recreate safe**: Dev/test can be destroyed anytime
- **Per-node config**: Each node has its own disk, network, patches
- **Dual-VLAN required**: Internal (111) + Services (20X)
- **HA etcd quorum**: 3 control planes validated

### Phase 2 (Current - GitOps Active âœ…)
- **ArgoCD App-of-Apps**: âœ… Root application deployed and managing services
- **Kustomize overlays**: âœ… Base + dev environment active
- **Branch per environment**: dev branch active, test/staging/main planned
- **Auto-sync**: âœ… Git push = automatic deployment (active)
- **Zero manual kubectl**: âœ… Full automation via Terraform + ArgoCD

### Archon Task Management
- Tasks tracked in Archon MCP server
- Sprint-based workflow (Sprints 1-11)
- Each task has validation criteria
- Mark tasks: todo â†’ doing â†’ review â†’ done

## Next Steps

**Sprint 1** (âœ… COMPLETED):
1. âœ… Task 1.1: Module structure - DONE
2. âœ… Task 1.2: Configure dev environment - DONE
3. âœ… Task 1.3: Apply Terraform and provision - DONE
4. âœ… Task 1.4: Validate cluster access - DONE
5. âœ… Bonus: VIP configuration - DONE
6. âœ… Bonus: Hostname configuration - DONE
7. âœ… Bonus: Worker node support - DONE
8. âœ… Bonus: Destroy/recreate validation - DONE

**Sprint 2** (âœ… COMPLETED):
- âœ… Task 2.1: Configure Helm provider in Terraform
- âœ… Task 2.2: Deploy Cilium v1.18.3 via Terraform Helm
- âœ… Task 2.3: Apply Terraform and deploy Cilium
- âœ… Task 2.4: Validate Cilium operational and network connectivity

**Sprint 3** (âœ… COMPLETED):
- âœ… Scale to 3 control planes HA (obsy, onyx, opale)
- âœ… Validate etcd quorum (3 members)
- âœ… Destroy/recreate workflow validated

**Sprint 4** (âœ… COMPLETED):
- âœ… Deploy ArgoCD v7.7.7 via Terraform Helm
- âœ… Create argocd/ structure (base + overlays/dev)
- âœ… Automate root-app bootstrap via kubectl provider
- âœ… Validate full GitOps workflow (zero manual kubectl)

**Sprint 5** (ğŸ”¨ IN PROGRESS):
- âœ… Replace MetalLB with Cilium L2 Announcements
- âœ… Deploy CiliumLoadBalancerIPPool (192.168.208.70-89)
- âœ… Validate LoadBalancer IP assignment
- ğŸ“… Next: Deploy Traefik Ingress

**After Sprint 5**:
- Sprint 6: cert-manager (TLS certificates)
- Sprint 7: Synology CSI (iSCSI storage)
- Sprints 8-11: Additional Phase 2 services

See `docs/ROADMAP.md` for complete sprint breakdown.
