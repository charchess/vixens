# Plan d'Impl√©mentation - Refactoring Terraform Infrastructure Vixens

**Date:** 2025-11-13
**Version:** 1.0
**Auteur:** Claude Code
**Objectif:** Optimisation DRY et best practices Terraform

---

## SOMMAIRE

1. [Vue d'Ensemble](#vue-densemble)
2. [√âtape 0: Pr√©paration](#√©tape-0-pr√©paration)
3. [√âtape 1: Cr√©ation Module Shared](#√©tape-1-cr√©ation-module-shared)
4. [√âtape 2: Typage et Restructuration Variables](#√©tape-2-typage-et-restructuration-variables)
5. [√âtape 3: Fusion et Optimisation ArgoCD](#√©tape-3-fusion-et-optimisation-argocd)
6. [√âtape 4: Optimisation Module Cilium](#√©tape-4-optimisation-module-cilium)
7. [√âtape 5: Suppression du Module Base](#√©tape-5-suppression-du-module-base)
8. [√âtape 6: S√©curisation Backend](#√©tape-6-s√©curisation-backend)
9. [√âtape 7: Validation Finale](#√©tape-7-validation-finale)
10. [Rollback Procedure](#rollback-procedure)

---

## VUE D'ENSEMBLE

### Architecture Actuelle (Probl√©matique)
```
terraform/
‚îú‚îÄ‚îÄ environments/dev/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf (30 lignes de pass-through)
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf (27 variables non typ√©es)
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars
‚îú‚îÄ‚îÄ base/ (‚ö†Ô∏è Wrapper inutile)
‚îÇ   ‚îú‚îÄ‚îÄ main.tf (redistribution variables)
‚îÇ   ‚îú‚îÄ‚îÄ argocd.tf (180 lignes DUPLIQU√âES)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ modules/
    ‚îú‚îÄ‚îÄ talos/ (‚úÖ Bon)
    ‚îú‚îÄ‚îÄ cilium/ (‚ö†Ô∏è Hardcoding)
    ‚îî‚îÄ‚îÄ argocd/ (‚ö†Ô∏è Dupliqu√© avec base/)
```

### Architecture Cible (Optimis√©e)
```
terraform/
‚îú‚îÄ‚îÄ environments/dev/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf (60 lignes, appels directs)
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf (8 objets typ√©s)
‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfvars
‚îÇ   ‚îî‚îÄ‚îÄ backend.tf
‚îî‚îÄ‚îÄ modules/
    ‚îú‚îÄ‚îÄ shared/ (‚ú® NOUVEAU - Configurations DRY)
    ‚îú‚îÄ‚îÄ talos/ (Inchang√©)
    ‚îú‚îÄ‚îÄ cilium/ (Optimis√©)
    ‚îî‚îÄ‚îÄ argocd/ (Optimis√© + fusion base/)
```

### M√©triques de Succ√®s
| M√©trique | Avant | Apr√®s | Objectif |
|----------|-------|-------|----------|
| Lignes de code | ~1400 | ~900 | -35% |
| Variables env | 27 | 8 objets | -70% |
| Duplication code | ArgoCD√ó2, tolerations√ó15 | 0 | -100% |
| Niveaux architecture | 3 | 2 | -33% |
| Variables typ√©es | 0% | 100% | +100% |

---

## √âTAPE 0: PR√âPARATION

### Objectif
S√©curiser l'infrastructure actuelle avant toute modification

### Dur√©e Estim√©e
15 minutes

### Actions D√©taill√©es

#### 0.1 Backup du State Terraform

```bash
cd /root/vixens/terraform/environments/dev

# Backup local state (si utilis√©)
cp terraform.tfstate terraform.tfstate.backup.$(date +%Y%m%d_%H%M%S) 2>/dev/null || true

# Backup remote state (S3/Minio)
terraform state pull > terraform.tfstate.backup.$(date +%Y%m%d_%H%M%S)

# V√©rifier le backup
ls -lah terraform.tfstate.backup.*
```

#### 0.2 Cr√©er Branche de Refactoring

```bash
cd /root/vixens

# V√©rifier √©tat git propre
git status

# Cr√©er branche d√©di√©e
git checkout -b terraform-refactor

# Tag de s√©curit√©
git tag -a pre-refactor-$(date +%Y%m%d) -m "State before Terraform refactoring"
git push origin pre-refactor-$(date +%Y%m%d)
```

#### 0.3 Snapshot de l'Infrastructure Actuelle

```bash
cd /root/vixens/terraform/environments/dev

# Plan actuel (doit √™tre vide si infrastructure stable)
terraform plan -out=pre-refactor.tfplan

# Sauvegarder les outputs
terraform output -json > outputs.pre-refactor.json

# Lister toutes les ressources
terraform state list > state.pre-refactor.txt
```

#### 0.4 Validation Pr√©-Refactoring

```bash
# Terraform doit √™tre valide
terraform validate

# Format correct
terraform fmt -check -recursive

# V√©rifier la sant√© du cluster
export KUBECONFIG=/root/vixens/terraform/environments/dev/kubeconfig-dev
kubectl get nodes
kubectl get pods -A | grep -v Running | grep -v Completed
```

### Crit√®res de Succ√®s
- ‚úÖ State sauvegard√© (fichier .backup pr√©sent)
- ‚úÖ Branche `terraform-refactor` cr√©√©e
- ‚úÖ `terraform plan` montre "No changes"
- ‚úÖ Cluster Kubernetes op√©rationnel
- ‚úÖ Tag git `pre-refactor-YYYYMMDD` cr√©√©

### Points de Vigilance
‚ö†Ô∏è **Ne PAS continuer si:**
- `terraform plan` montre des changements non appliqu√©s
- Le cluster a des pods en erreur
- √âtat git non propre (fichiers non commit√©s)

---

## √âTAPE 1: CR√âATION MODULE SHARED

### Objectif
Cr√©er un module central pour toutes les configurations communes et r√©utilisables

### Dur√©e Estim√©e
30 minutes

### Structure du Module

```
modules/shared/
‚îú‚îÄ‚îÄ locals.tf           # Configurations locales DRY
‚îú‚îÄ‚îÄ outputs.tf          # Exports pour autres modules
‚îú‚îÄ‚îÄ variables.tf        # Inputs du module
‚îî‚îÄ‚îÄ versions.tf         # Provider requirements
```

### Actions D√©taill√©es

#### 1.1 Cr√©er la Structure

```bash
cd /root/vixens/terraform/modules
mkdir -p shared
cd shared
touch locals.tf outputs.tf variables.tf versions.tf
```

#### 1.2 Cr√©er `versions.tf`

**Fichier:** `modules/shared/versions.tf`

```hcl
terraform {
  required_version = ">= 1.5.0"

  # Aucun provider requis - module de donn√©es uniquement
  required_providers {}
}
```

#### 1.3 Cr√©er `variables.tf`

**Fichier:** `modules/shared/variables.tf`

```hcl
variable "environment" {
  description = "Deployment environment (dev, test, staging, prod)"
  type        = string

  validation {
    condition     = contains(["dev", "test", "staging", "prod"], var.environment)
    error_message = "Environment must be one of: dev, test, staging, prod"
  }
}

variable "loadbalancer_ip" {
  description = "LoadBalancer IP address for Cilium annotations (optional)"
  type        = string
  default     = null
}
```

#### 1.4 Cr√©er `locals.tf` - Configuration Centralis√©e

**Fichier:** `modules/shared/locals.tf`

```hcl
# ============================================================================
# SHARED CONFIGURATIONS - DRY Principle
# ============================================================================
# Ce module centralise toutes les configurations r√©p√©t√©es dans l'infrastructure
# pour √©liminer la duplication et faciliter la maintenance.

locals {
  # --------------------------------------------------------------------------
  # CHART VERSIONS - Centralisation des versions Helm
  # --------------------------------------------------------------------------
  chart_versions = {
    cilium  = "1.18.3"
    argocd  = "7.7.7"
    traefik = "2.10.5"
  }

  # --------------------------------------------------------------------------
  # KUBERNETES TOLERATIONS - Control Plane
  # --------------------------------------------------------------------------
  # Tol√©rations pour scheduler les pods sur les control planes
  # Utilis√© par: ArgoCD, Cilium Hubble, Cilium Operator
  control_plane_tolerations = [
    {
      key      = "node-role.kubernetes.io/control-plane"
      operator = "Exists"
      effect   = "NoSchedule"
    }
  ]

  # --------------------------------------------------------------------------
  # CILIUM ANNOTATIONS
  # --------------------------------------------------------------------------
  # Annotations pour LoadBalancer avec Cilium LB IPAM
  cilium_lb_annotations = var.loadbalancer_ip != null ? {
    "io.cilium/lb-ipam-ips" = var.loadbalancer_ip
  } : {}

  # --------------------------------------------------------------------------
  # ENVIRONMENT-SPECIFIC CONFIGURATIONS
  # --------------------------------------------------------------------------
  # Configurations qui varient par environnement
  env_config = {
    dev = {
      argocd_replicas = 1
      cilium_replicas = 1
      log_level       = "debug"
      retention_days  = 7
    }
    test = {
      argocd_replicas = 1
      cilium_replicas = 1
      log_level       = "info"
      retention_days  = 14
    }
    staging = {
      argocd_replicas = 2
      cilium_replicas = 2
      log_level       = "info"
      retention_days  = 30
    }
    prod = {
      argocd_replicas = 3
      cilium_replicas = 2
      log_level       = "warn"
      retention_days  = 90
    }
  }

  # Configuration active bas√©e sur l'environnement
  active_env_config = local.env_config[var.environment]

  # --------------------------------------------------------------------------
  # COMMON LABELS
  # --------------------------------------------------------------------------
  # Labels Kubernetes standardis√©s (https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)
  common_labels = {
    "app.kubernetes.io/managed-by" = "terraform"
    "app.kubernetes.io/part-of"    = "vixens-homelab"
    "environment"                  = var.environment
  }

  # --------------------------------------------------------------------------
  # NETWORKING CONSTANTS
  # --------------------------------------------------------------------------
  network = {
    pod_subnet     = "10.244.0.0/16"
    service_subnet = "10.96.0.0/12"

    # VLAN IDs par environnement
    vlan_internal = 111
    vlan_services = {
      dev     = 208
      test    = 209
      staging = 210
      prod    = 201
    }
  }

  # --------------------------------------------------------------------------
  # SECURITY POLICIES
  # --------------------------------------------------------------------------
  security = {
    # Capacit√©s Linux requises pour Cilium Agent
    cilium_agent_capabilities = [
      "CHOWN",
      "KILL",
      "NET_ADMIN",
      "NET_RAW",
      "IPC_LOCK",
      "SYS_ADMIN",
      "SYS_RESOURCE",
      "DAC_OVERRIDE",
      "FOWNER",
      "SETGID",
      "SETUID"
    ]

    # Capacit√©s pour cleanCiliumState
    cilium_clean_capabilities = [
      "NET_ADMIN",
      "SYS_ADMIN",
      "SYS_RESOURCE"
    ]
  }

  # --------------------------------------------------------------------------
  # TIMEOUTS
  # --------------------------------------------------------------------------
  timeouts = {
    helm_install       = 600  # 10 minutes
    health_check       = 300  # 5 minutes
    pod_ready          = 120  # 2 minutes
    service_available  = 180  # 3 minutes
  }
}
```

#### 1.5 Cr√©er `outputs.tf` - Exports

**Fichier:** `modules/shared/outputs.tf`

```hcl
# ============================================================================
# SHARED MODULE OUTPUTS
# ============================================================================

# --------------------------------------------------------------------------
# Chart Versions
# --------------------------------------------------------------------------
output "chart_versions" {
  description = "Centralized Helm chart versions"
  value       = local.chart_versions
}

output "cilium_version" {
  description = "Cilium chart version"
  value       = local.chart_versions.cilium
}

output "argocd_version" {
  description = "ArgoCD chart version"
  value       = local.chart_versions.argocd
}

# --------------------------------------------------------------------------
# Tolerations
# --------------------------------------------------------------------------
output "control_plane_tolerations" {
  description = "Standard tolerations for control plane scheduling"
  value       = local.control_plane_tolerations
}

# --------------------------------------------------------------------------
# Annotations
# --------------------------------------------------------------------------
output "cilium_lb_annotations" {
  description = "Cilium LoadBalancer IPAM annotations"
  value       = local.cilium_lb_annotations
}

# --------------------------------------------------------------------------
# Environment Configuration
# --------------------------------------------------------------------------
output "env_config" {
  description = "Active environment-specific configuration"
  value       = local.active_env_config
}

# --------------------------------------------------------------------------
# Labels
# --------------------------------------------------------------------------
output "common_labels" {
  description = "Standard Kubernetes labels"
  value       = local.common_labels
}

# --------------------------------------------------------------------------
# Network Configuration
# --------------------------------------------------------------------------
output "network" {
  description = "Network configuration (subnets, VLANs)"
  value       = local.network
}

output "vlan_services" {
  description = "VLAN ID for services in current environment"
  value       = local.network.vlan_services[var.environment]
}

# --------------------------------------------------------------------------
# Security
# --------------------------------------------------------------------------
output "cilium_agent_capabilities" {
  description = "Linux capabilities for Cilium agent"
  value       = local.security.cilium_agent_capabilities
}

output "cilium_clean_capabilities" {
  description = "Linux capabilities for cleanCiliumState"
  value       = local.security.cilium_clean_capabilities
}

# --------------------------------------------------------------------------
# Timeouts
# --------------------------------------------------------------------------
output "timeouts" {
  description = "Standard timeouts for operations"
  value       = local.timeouts
}
```

#### 1.6 Tester le Module Shared

```bash
cd /root/vixens/terraform/modules/shared

# Valider la syntaxe
terraform init
terraform validate

# Format
terraform fmt

# Test avec une configuration temporaire
cat > test.tf <<'EOF'
module "shared_test" {
  source = "."

  environment     = "dev"
  loadbalancer_ip = "192.168.208.71"
}

output "test_chart_versions" {
  value = module.shared_test.chart_versions
}

output "test_tolerations" {
  value = module.shared_test.control_plane_tolerations
}
EOF

terraform init
terraform plan
terraform apply -auto-approve

# V√©rifier les outputs
terraform output

# Nettoyer
rm -f test.tf
rm -rf .terraform* terraform.tfstate*
```

### Crit√®res de Succ√®s
- ‚úÖ Module `shared/` cr√©√© avec 4 fichiers
- ‚úÖ `terraform validate` r√©ussit
- ‚úÖ Test du module montre les outputs corrects
- ‚úÖ Aucune duplication de code

### Points de Vigilance
‚ö†Ô∏è **Important:**
- Le module `shared` ne doit contenir AUCUNE ressource Terraform
- Uniquement des `locals` et `outputs`
- Aucun provider requis (module de donn√©es uniquement)

---

## √âTAPE 2: TYPAGE ET RESTRUCTURATION VARIABLES

### Objectif
Transformer les 27 variables plates non typ√©es en 8 objets structur√©s avec validation

### Dur√©e Estim√©e
45 minutes

### Actions D√©taill√©es

#### 2.1 Cr√©er Nouveau `variables.tf` pour Dev

**Fichier:** `environments/dev/variables.tf` (REMPLACER COMPL√àTEMENT)

```hcl
# ============================================================================
# ENVIRONMENT VARIABLES - DEV
# ============================================================================
# Variables typ√©es et structur√©es selon les best practices Terraform

# --------------------------------------------------------------------------
# ENVIRONMENT METADATA
# --------------------------------------------------------------------------
variable "environment" {
  description = "Deployment environment name"
  type        = string
  default     = "dev"

  validation {
    condition     = contains(["dev", "test", "staging", "prod"], var.environment)
    error_message = "Environment must be one of: dev, test, staging, prod"
  }
}

variable "git_branch" {
  description = "Git branch for ArgoCD applications synchronization"
  type        = string
  default     = "dev"

  validation {
    condition     = contains(["dev", "test", "staging", "main"], var.git_branch)
    error_message = "Branch must be one of: dev, test, staging, main"
  }
}

# --------------------------------------------------------------------------
# CLUSTER CONFIGURATION
# --------------------------------------------------------------------------
variable "cluster" {
  description = "Talos/Kubernetes cluster configuration"
  type = object({
    name               = string
    endpoint           = string
    vip                = string
    talos_version      = string
    talos_image        = string
    kubernetes_version = string
  })

  validation {
    condition     = can(regex("^https://", var.cluster.endpoint))
    error_message = "Cluster endpoint must start with https://"
  }

  validation {
    condition     = can(regex("^v[0-9]+\\.[0-9]+\\.[0-9]+$", var.cluster.talos_version))
    error_message = "Talos version must be in format v1.11.5"
  }
}

# --------------------------------------------------------------------------
# NODE CONFIGURATION
# --------------------------------------------------------------------------
variable "control_plane_nodes" {
  description = "Map of control plane nodes with complete configuration"
  type = map(object({
    name         = string
    ip_address   = string
    mac_address  = string
    install_disk = string
    network = object({
      interface = string
      vlans = list(object({
        vlanId    = number
        addresses = list(string)
        gateway   = string
      }))
    })
  }))

  validation {
    condition     = length(var.control_plane_nodes) % 2 == 1
    error_message = "Control plane nodes count must be odd (etcd quorum requirement: 1, 3, 5, etc.)"
  }
}

variable "worker_nodes" {
  description = "Map of worker nodes with complete configuration"
  type = map(object({
    name         = string
    ip_address   = string
    mac_address  = string
    install_disk = string
    network = object({
      interface = string
      vlans = list(object({
        vlanId    = number
        addresses = list(string)
        gateway   = string
      }))
    })
  }))
  default = {}
}

# --------------------------------------------------------------------------
# ARGOCD CONFIGURATION
# --------------------------------------------------------------------------
variable "argocd" {
  description = "ArgoCD GitOps configuration"
  type = object({
    service_type      = string
    loadbalancer_ip   = string
    hostname          = string
    insecure          = bool
    disable_auth      = bool
    anonymous_enabled = bool
  })

  validation {
    condition     = contains(["ClusterIP", "LoadBalancer"], var.argocd.service_type)
    error_message = "ArgoCD service type must be ClusterIP or LoadBalancer"
  }

  validation {
    condition     = can(regex("^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$", var.argocd.loadbalancer_ip))
    error_message = "ArgoCD LoadBalancer IP must be a valid IPv4 address"
  }
}

# --------------------------------------------------------------------------
# CILIUM L2 ANNOUNCEMENT CONFIGURATION
# --------------------------------------------------------------------------
variable "cilium_l2" {
  description = "Cilium L2 Announcement and LoadBalancer IP Pool configuration"
  type = object({
    pool_name    = string
    pool_ips     = list(string)
    policy_name  = string
    interfaces   = list(string)
    node_selector = map(string)
  })

  validation {
    condition     = length(var.cilium_l2.pool_ips) > 0
    error_message = "At least one IP range must be specified for Cilium L2 pool"
  }
}

# --------------------------------------------------------------------------
# NETWORK CONFIGURATION
# --------------------------------------------------------------------------
variable "network" {
  description = "Network configuration for the environment"
  type = object({
    vlan_services_subnet = string
  })

  validation {
    condition     = can(cidrhost(var.network.vlan_services_subnet, 0))
    error_message = "VLAN services subnet must be a valid CIDR (e.g., 192.168.208.0/24)"
  }
}

# --------------------------------------------------------------------------
# FILE PATHS
# --------------------------------------------------------------------------
variable "paths" {
  description = "File paths for generated configurations and manifests"
  type = object({
    kubeconfig              = string
    talosconfig             = string
    cilium_ip_pool_yaml     = string
    cilium_l2_policy_yaml   = string
  })

  default = {
    kubeconfig            = "./kubeconfig-dev"
    talosconfig           = "./talosconfig-dev"
    cilium_ip_pool_yaml   = "../../../apps/cilium-lb/overlays/dev/ippool.yaml"
    cilium_l2_policy_yaml = "../../../apps/cilium-lb/base/l2policy.yaml"
  }
}
```

#### 2.2 Cr√©er Nouveau `terraform.tfvars` pour Dev

**Fichier:** `environments/dev/terraform.tfvars` (REMPLACER COMPL√àTEMENT)

```hcl
# ============================================================================
# TERRAFORM VARIABLES - DEV ENVIRONMENT
# ============================================================================

# --------------------------------------------------------------------------
# Environment Metadata
# --------------------------------------------------------------------------
environment = "dev"
git_branch  = "dev"

# --------------------------------------------------------------------------
# Cluster Configuration
# --------------------------------------------------------------------------
cluster = {
  name               = "vixens-dev"
  endpoint           = "https://192.168.111.160:6443"
  vip                = "192.168.111.160"
  talos_version      = "v1.11.5"
  talos_image        = "factory.talos.dev/installer/613e1592b2da41ae5e265e8789429f22e121aab91cb4deb6bc3c0b6262961245:v1.11.5"
  kubernetes_version = "1.30.0"
}

# --------------------------------------------------------------------------
# Control Plane Nodes (3 nodes HA)
# --------------------------------------------------------------------------
control_plane_nodes = {
  "obsy" = {
    name         = "obsy"
    ip_address   = "192.168.0.162"
    mac_address  = "00:15:5D:00:CB:10"
    install_disk = "/dev/sda"
    network = {
      interface = "enx00155d00cb10"
      vlans = [
        {
          vlanId    = 111
          addresses = ["192.168.111.162/24"]
          gateway   = ""
        },
        {
          vlanId    = 208
          addresses = ["192.168.208.162/24"]
          gateway   = "192.168.208.1"
        }
      ]
    }
  },
  "onyx" = {
    name         = "onyx"
    ip_address   = "192.168.0.164"
    mac_address  = "00:15:5D:00:CB:11"
    install_disk = "/dev/sda"
    network = {
      interface = "enx00155d00cb11"
      vlans = [
        {
          vlanId    = 111
          addresses = ["192.168.111.164/24"]
          gateway   = ""
        },
        {
          vlanId    = 208
          addresses = ["192.168.208.164/24"]
          gateway   = "192.168.208.1"
        }
      ]
    }
  },
  "opale" = {
    name         = "opale"
    ip_address   = "192.168.0.163"
    mac_address  = "00:15:5D:00:CB:0B"
    install_disk = "/dev/sda"
    network = {
      interface = "enx00155d00cb0b"
      vlans = [
        {
          vlanId    = 111
          addresses = ["192.168.111.163/24"]
          gateway   = ""
        },
        {
          vlanId    = 208
          addresses = ["192.168.208.163/24"]
          gateway   = "192.168.208.1"
        }
      ]
    }
  }
}

# No worker nodes in dev
worker_nodes = {}

# --------------------------------------------------------------------------
# ArgoCD Configuration
# --------------------------------------------------------------------------
argocd = {
  service_type      = "LoadBalancer"
  loadbalancer_ip   = "192.168.208.71"
  hostname          = "argocd.dev.truxonline.com"
  insecure          = true
  disable_auth      = true
  anonymous_enabled = true
}

# --------------------------------------------------------------------------
# Cilium L2 Announcement
# --------------------------------------------------------------------------
cilium_l2 = {
  pool_name  = "dev-pool"
  pool_ips   = ["192.168.208.70-192.168.208.89"]
  policy_name = "dev-l2-policy"
  interfaces = ["eth1"]
  node_selector = {
    "kubernetes.io/hostname" = "obsy"
  }
}

# --------------------------------------------------------------------------
# Network Configuration
# --------------------------------------------------------------------------
network = {
  vlan_services_subnet = "192.168.208.0/24"
}

# --------------------------------------------------------------------------
# File Paths (using defaults from variables.tf)
# --------------------------------------------------------------------------
# paths = {
#   kubeconfig            = "./kubeconfig-dev"
#   talosconfig           = "./talosconfig-dev"
#   cilium_ip_pool_yaml   = "../../../apps/cilium-lb/overlays/dev/ippool.yaml"
#   cilium_l2_policy_yaml = "../../../apps/cilium-lb/base/l2policy.yaml"
# }
```

#### 2.3 Validation du Typage

```bash
cd /root/vixens/terraform/environments/dev

# Backup ancien fichier
cp variables.tf variables.tf.old
cp terraform.tfvars terraform.tfvars.old

# Remplacer avec les nouveaux fichiers (cr√©√©s ci-dessus)
# ... (via Write tool)

# Valider
terraform validate

# Devrait √©chouer car main.tf utilise encore l'ancienne syntaxe
# C'est normal - on corrige √ßa √† l'√©tape 5
```

### Crit√®res de Succ√®s
- ‚úÖ 27 variables plates ‚Üí 8 objets structur√©s
- ‚úÖ 100% des variables typ√©es
- ‚úÖ Validations sur formats (IP, versions, CIDR)
- ‚úÖ Validation quorum etcd (nombre impair)

---

## √âTAPE 3: FUSION ET OPTIMISATION ARGOCD

### Objectif
Fusionner `base/argocd.tf` et `modules/argocd/main.tf` en un seul module optimis√©

### Dur√©e Estim√©e
40 minutes

### Actions D√©taill√©es

#### 3.1 Analyser les Diff√©rences

**Diff√©rences entre `base/argocd.tf` et `modules/argocd/main.tf`:**

| Aspect | base/argocd.tf | modules/argocd/ |
|--------|----------------|-----------------|
| Service annotations | `io.cilium/lb-ipam-ips` | Utilise `loadBalancerIP` |
| Template path | Hardcod√© `../../../argocd/base/root-app.yaml.tpl` | Variable `root_app_template_path` |
| Variables | Utilise vars de base/ | Utilise vars propres |
| D√©pendances | `module.cilium` | `var.cilium_module` |

**D√©cision:** Utiliser `modules/argocd/` comme base et enrichir avec fonctionnalit√©s de `base/`

#### 3.2 Optimiser `modules/argocd/variables.tf`

**Fichier:** `modules/argocd/variables.tf` (REMPLACER)

```hcl
# ============================================================================
# ARGOCD MODULE VARIABLES
# ============================================================================

variable "environment" {
  description = "Deployment environment (dev, test, staging, prod)"
  type        = string
}

variable "git_branch" {
  description = "Git branch for ArgoCD applications"
  type        = string
}

variable "chart_version" {
  description = "ArgoCD Helm chart version"
  type        = string
}

variable "namespace" {
  description = "Kubernetes namespace for ArgoCD"
  type        = string
  default     = "argocd"
}

variable "argocd_config" {
  description = "ArgoCD configuration object"
  type = object({
    service_type      = string
    loadbalancer_ip   = string
    hostname          = string
    insecure          = bool
    disable_auth      = bool
    anonymous_enabled = bool
  })
}

variable "root_app_template_path" {
  description = "Path to root-app template file"
  type        = string
}

variable "control_plane_tolerations" {
  description = "Tolerations for control plane scheduling"
  type = list(object({
    key      = string
    operator = string
    effect   = string
  }))
}

variable "cilium_module" {
  description = "Cilium module dependency (for depends_on)"
  type        = any
}

variable "common_labels" {
  description = "Common labels to apply to resources"
  type        = map(string)
  default     = {}
}
```

#### 3.3 Optimiser `modules/argocd/main.tf`

**Fichier:** `modules/argocd/main.tf` (REMPLACER COMPL√àTEMENT)

```hcl
# ============================================================================
# ARGOCD MODULE - GitOps Deployment
# ============================================================================
# D√©ploie ArgoCD avec Helm et bootstrap le root-app pour App-of-Apps pattern

resource "helm_release" "argocd" {
  name             = "argocd"
  repository       = "https://argoproj.github.io/argo-helm"
  chart            = "argo-cd"
  version          = var.chart_version
  namespace        = var.namespace
  create_namespace = true

  wait          = true
  wait_for_jobs = true
  timeout       = 600 # 10 minutes

  values = [yamlencode({
    # -------------------------------------------------------------------------
    # Server Configuration
    # -------------------------------------------------------------------------
    server = {
      config = {
        url = "http://${var.argocd_config.loadbalancer_ip}"
      }

      extraArgs = concat(
        var.argocd_config.insecure ? ["--insecure"] : [],
        var.argocd_config.disable_auth ? ["--disable-auth"] : []
      )

      # Service configuration avec Cilium LB IPAM
      service = {
        type = var.argocd_config.service_type
        annotations = merge(
          var.common_labels,
          {
            "environment" = var.environment
          },
          # Annotation Cilium pour IP assignment
          var.argocd_config.service_type == "LoadBalancer" ? {
            "io.cilium/lb-ipam-ips" = var.argocd_config.loadbalancer_ip
          } : {}
        )
      }

      # Ingress (d√©sactiv√© - utilis√© plus tard avec Traefik)
      ingress = {
        enabled          = false
        ingressClassName = "traefik"
        hosts            = [var.argocd_config.hostname]
        paths            = ["/"]
        annotations = {
          "traefik.ingress.kubernetes.io/router.entrypoints" = "web"
        }
      }

      # Tolerations control-plane (DRY via module shared)
      tolerations = var.control_plane_tolerations
    }

    # -------------------------------------------------------------------------
    # Components Tolerations (DRY)
    # -------------------------------------------------------------------------
    repoServer      = { tolerations = var.control_plane_tolerations }
    controller      = { tolerations = var.control_plane_tolerations }
    redis           = { tolerations = var.control_plane_tolerations }
    applicationSet  = { tolerations = var.control_plane_tolerations }
    notifications   = { tolerations = var.control_plane_tolerations }
    redisSecretInit = { tolerations = var.control_plane_tolerations }

    # -------------------------------------------------------------------------
    # Dex (SSO) - Disabled
    # -------------------------------------------------------------------------
    dex = {
      enabled = false
    }

    # -------------------------------------------------------------------------
    # Configuration
    # -------------------------------------------------------------------------
    configs = {
      params = {
        "server.insecure" = var.argocd_config.insecure
      }

      cm = {
        "users.anonymous.enabled" = var.argocd_config.anonymous_enabled ? "true" : "false"
        "url"                     = "http://${var.argocd_config.loadbalancer_ip}"

        # RBAC Policy
        "policy.csv" = <<-EOT
          p, role:readonly, applications, get, */*, allow
          p, role:readonly, applications, list, */*, allow
          p, role:readonly, clusters, get, *, allow
          p, role:readonly, repositories, get, *, allow
          p, role:readonly, repositories, list, *, allow
          p, role:readonly, projects, get, *, allow
          p, role:readonly, projects, list, *, allow
          g, anonymous, role:readonly
          g, default, role:readonly
        EOT
      }

      rbac = {
        create        = true
        policyDefault = "role:readonly"
      }
    }
  })]

  # D√©ployer apr√®s Cilium (pour LB IPAM)
  depends_on = [
    var.cilium_module
  ]
}

# ============================================================================
# ROOT APPLICATION - App-of-Apps Bootstrap
# ============================================================================
# Bootstrap automatique du root-app pour GitOps complet
# Apr√®s ce d√©ploiement, toutes les modifications se font via Git

resource "kubectl_manifest" "argocd_root_app" {
  yaml_body = templatefile(var.root_app_template_path, {
    environment     = var.environment
    target_revision = var.git_branch
    overlay_path    = "argocd/overlays/${var.environment}"
  })

  depends_on = [
    helm_release.argocd
  ]
}
```

#### 3.4 Ajouter `modules/argocd/versions.tf`

**Fichier:** `modules/argocd/versions.tf` (COMPL√âTER si manquant)

```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = "~> 1.19.0"
    }
  }
}
```

### Crit√®res de Succ√®s
- ‚úÖ Module ArgoCD optimis√© avec objets typ√©s
- ‚úÖ Tol√©rations via variable (DRY)
- ‚úÖ Annotations Cilium correctement g√©r√©es
- ‚úÖ Aucune duplication de code

---

## √âTAPE 4: OPTIMISATION MODULE CILIUM

### Objectif
Supprimer hardcoding et utiliser configurations du module shared

### Dur√©e Estim√©e
30 minutes

### Actions D√©taill√©es

#### 4.1 Optimiser `modules/cilium/variables.tf`

**Fichier:** `modules/cilium/variables.tf` (REMPLACER)

```hcl
# ============================================================================
# CILIUM MODULE VARIABLES
# ============================================================================

variable "release_name" {
  description = "Helm release name"
  type        = string
  default     = "cilium"
}

variable "chart_version" {
  description = "Cilium Helm chart version"
  type        = string
}

variable "namespace" {
  description = "Kubernetes namespace for Cilium"
  type        = string
  default     = "kube-system"
}

variable "kubeconfig_path" {
  description = "Path to kubeconfig file"
  type        = string
}

variable "ip_pool_yaml_path" {
  description = "Path to CiliumLoadBalancerIPPool YAML manifest"
  type        = string
}

variable "l2_policy_yaml_path" {
  description = "Path to CiliumL2AnnouncementPolicy YAML manifest"
  type        = string
}

variable "talos_cluster_module" {
  description = "Talos cluster module dependency (for depends_on)"
  type        = any
}

variable "wait_for_k8s_api" {
  description = "Wait for Kubernetes API readiness resource"
  type        = any
}

variable "cilium_agent_capabilities" {
  description = "Linux capabilities for Cilium agent"
  type        = list(string)
}

variable "cilium_clean_capabilities" {
  description = "Linux capabilities for cleanCiliumState"
  type        = list(string)
}

variable "control_plane_tolerations" {
  description = "Tolerations for control plane scheduling"
  type = list(object({
    key      = string
    operator = string
    effect   = string
  }))
}

variable "timeout" {
  description = "Helm install timeout in seconds"
  type        = number
  default     = 600
}
```

#### 4.2 Optimiser `modules/cilium/main.tf`

**Fichier:** `modules/cilium/main.tf` (REMPLACER sections concern√©es)

```hcl
# ============================================================================
# CILIUM MODULE - CNI Deployment
# ============================================================================

resource "helm_release" "cilium" {
  name       = var.release_name
  repository = "https://helm.cilium.io/"
  chart      = "cilium"
  version    = var.chart_version # ‚úÖ Plus de hardcoding
  namespace  = var.namespace

  wait          = true
  wait_for_jobs = true
  timeout       = var.timeout # ‚úÖ Configurable

  values = [yamlencode({
    kubeProxyReplacement = true
    k8sServiceHost       = "localhost"
    k8sServicePort       = 7445

    # L2 Announcements pour LoadBalancer
    l2announcements = {
      enabled = true
    }

    k8sClientRateLimit = {
      qps   = 10
      burst = 20
    }

    externalIPs = {
      enabled = true
    }

    ipam = {
      mode = "kubernetes"
    }

    routingMode    = "tunnel"
    tunnelProtocol = "vxlan"

    # ‚úÖ Security context via variables (DRY)
    securityContext = {
      capabilities = {
        ciliumAgent      = var.cilium_agent_capabilities
        cleanCiliumState = var.cilium_clean_capabilities
      }
    }

    # Talos-specific cgroup configuration
    cgroup = {
      autoMount = {
        enabled = false
      }
      hostRoot = "/sys/fs/cgroup"
    }

    bpf = {
      hostLegacyRouting = true
    }

    # Hubble observability
    hubble = {
      enabled = true

      relay = {
        enabled     = true
        tolerations = var.control_plane_tolerations # ‚úÖ DRY
      }

      ui = {
        enabled     = true
        tolerations = var.control_plane_tolerations # ‚úÖ DRY
      }

      metrics = {
        enabled = [
          "dns",
          "drop",
          "tcp",
          "flow",
          "port-distribution",
          "icmp",
          "http"
        ]
      }
    }

    # Operator configuration
    operator = {
      replicas    = 1
      tolerations = var.control_plane_tolerations # ‚úÖ DRY
    }
  })]

  depends_on = [
    var.talos_cluster_module,
    var.wait_for_k8s_api
  ]
}

# Rest of the file unchanged (wait_for_cilium_crds, kubectl_manifest, etc.)
# ...
```

### Crit√®res de Succ√®s
- ‚úÖ Chart version en variable (plus de hardcoding)
- ‚úÖ Capabilities via variables
- ‚úÖ Tolerations DRY
- ‚úÖ Timeout configurable

---

## √âTAPE 5: SUPPRESSION DU MODULE BASE

### Objectif
√âliminer le niveau d'abstraction inutile et appeler directement les modules

### Dur√©e Estim√©e
60 minutes

### Actions D√©taill√©es

#### 5.1 Cr√©er Nouveau `environments/dev/main.tf`

**Fichier:** `environments/dev/main.tf` (REMPLACER COMPL√àTEMENT)

```hcl
# ============================================================================
# VIXENS HOMELAB - DEV ENVIRONMENT
# ============================================================================
# Infrastructure Terraform pour cluster Kubernetes dev (3 control planes HA)

terraform {
  required_version = ">= 1.5.0"
}

# ============================================================================
# LOCALS - Computed Values
# ============================================================================

locals {
  # Repository root pour paths absolus
  repo_root = abspath("${path.module}/../../..")

  # Paths dynamiques pour manifests Cilium
  cilium_manifests = {
    ip_pool   = "${local.repo_root}/apps/cilium-lb/overlays/${var.environment}/ippool.yaml"
    l2_policy = "${local.repo_root}/apps/cilium-lb/base/l2policy.yaml"
  }

  # Template path pour ArgoCD root-app
  argocd_root_app_template = "${local.repo_root}/argocd/base/root-app.yaml.tpl"
}

# ============================================================================
# MODULE SHARED - DRY Configurations
# ============================================================================

module "shared" {
  source = "../../modules/shared"

  environment     = var.environment
  loadbalancer_ip = var.argocd.loadbalancer_ip
}

# ============================================================================
# MODULE TALOS - Cluster Infrastructure
# ============================================================================

module "talos_cluster" {
  source = "../../modules/talos"

  cluster_name        = var.cluster.name
  cluster_endpoint    = var.cluster.endpoint
  talos_version       = var.cluster.talos_version
  talos_image         = var.cluster.talos_image
  kubernetes_version  = var.cluster.kubernetes_version
  control_plane_nodes = var.control_plane_nodes
  worker_nodes        = var.worker_nodes
}

# ============================================================================
# LOCAL FILES - Kubeconfig & Talosconfig
# ============================================================================

resource "local_file" "kubeconfig" {
  content         = module.talos_cluster.kubeconfig
  filename        = var.paths.kubeconfig
  file_permission = "0600"
}

resource "local_file" "talosconfig" {
  content         = module.talos_cluster.talosconfig
  filename        = var.paths.talosconfig
  file_permission = "0600"
}

# ============================================================================
# WAIT FOR K8S API - Readiness Check
# ============================================================================

resource "null_resource" "wait_for_k8s_api" {
  depends_on = [module.talos_cluster]

  provisioner "local-exec" {
    command = <<-EOT
      end_time=$(( $(date +%s) + 300 )) # 5 minutes timeout
      while true; do
        http_code=$(curl -k -s -o /dev/null -w "%%{http_code}" "$K8S_ENDPOINT/version")
        if [ "$http_code" = "401" ] || [ "$http_code" = "200" ]; then
          echo "Kubernetes API ready at $K8S_ENDPOINT (HTTP $http_code)"
          break
        fi

        if [ "$(date +%s)" -gt "$end_time" ]; then
          echo "Timeout: Kubernetes API not ready after 5 minutes (HTTP $http_code)"
          exit 1
        fi

        echo "Waiting for K8s API... (HTTP $http_code)"
        sleep 10
      done
    EOT

    environment = {
      K8S_ENDPOINT = module.talos_cluster.kubernetes_host
    }
  }
}

# ============================================================================
# MODULE CILIUM - CNI Deployment
# ============================================================================

module "cilium" {
  source = "../../modules/cilium"

  release_name  = "cilium"
  chart_version = module.shared.cilium_version
  namespace     = "kube-system"

  kubeconfig_path     = local_file.kubeconfig.filename
  ip_pool_yaml_path   = local.cilium_manifests.ip_pool
  l2_policy_yaml_path = local.cilium_manifests.l2_policy

  # DRY configurations from shared module
  cilium_agent_capabilities = module.shared.cilium_agent_capabilities
  cilium_clean_capabilities = module.shared.cilium_clean_capabilities
  control_plane_tolerations = module.shared.control_plane_tolerations
  timeout                   = module.shared.timeouts.helm_install

  talos_cluster_module = module.talos_cluster
  wait_for_k8s_api     = null_resource.wait_for_k8s_api
}

# ============================================================================
# MODULE ARGOCD - GitOps Deployment
# ============================================================================

module "argocd" {
  source = "../../modules/argocd"

  environment             = var.environment
  git_branch              = var.git_branch
  chart_version           = module.shared.argocd_version
  namespace               = "argocd"
  argocd_config           = var.argocd
  root_app_template_path  = local.argocd_root_app_template

  # DRY configurations
  control_plane_tolerations = module.shared.control_plane_tolerations
  common_labels             = module.shared.common_labels

  cilium_module = module.cilium
}

# ============================================================================
# OUTPUTS
# ============================================================================

output "cluster_endpoint" {
  description = "Kubernetes API endpoint"
  value       = module.talos_cluster.cluster_endpoint
}

output "kubeconfig_path" {
  description = "Path to kubeconfig file"
  value       = local_file.kubeconfig.filename
}

output "talosconfig_path" {
  description = "Path to talosconfig file"
  value       = local_file.talosconfig.filename
}

output "argocd_url" {
  description = "ArgoCD UI URL"
  value       = "http://${var.argocd.loadbalancer_ip}"
}
```

#### 5.2 Cr√©er `environments/dev/providers.tf`

**Fichier:** `environments/dev/providers.tf` (NOUVEAU)

```hcl
# ============================================================================
# TERRAFORM PROVIDERS CONFIGURATION
# ============================================================================

# Provider Helm - pour d√©ploiements Helm charts (Cilium, ArgoCD)
provider "helm" {
  kubernetes {
    host                   = module.talos_cluster.kubernetes_host
    client_certificate     = base64decode(module.talos_cluster.kubernetes_client_certificate)
    client_key             = base64decode(module.talos_cluster.kubernetes_client_key)
    cluster_ca_certificate = base64decode(module.talos_cluster.kubernetes_ca_certificate)
  }
}

# Provider kubectl - pour manifests Kubernetes (CRDs Cilium, ArgoCD root-app)
provider "kubectl" {
  host                   = module.talos_cluster.kubernetes_host
  client_certificate     = base64decode(module.talos_cluster.kubernetes_client_certificate)
  client_key             = base64decode(module.talos_cluster.kubernetes_client_key)
  cluster_ca_certificate = base64decode(module.talos_cluster.kubernetes_ca_certificate)
  load_config_file       = false
}

# Provider Kubernetes - pour ressources natives (si n√©cessaire)
provider "kubernetes" {
  config_path = local_file.kubeconfig.filename
}
```

#### 5.3 Cr√©er `environments/dev/versions.tf`

**Fichier:** `environments/dev/versions.tf` (NOUVEAU si manquant)

```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    talos = {
      source  = "siderolabs/talos"
      version = "~> 0.9"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.4"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = "~> 1.19.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.25"
    }
    null = {
      source  = "hashicorp/null"
      version = "~> 3.2"
    }
  }
}
```

#### 5.4 Supprimer le R√©pertoire `base/`

```bash
cd /root/vixens/terraform

# Backup avant suppression
cp -r base base.backup.$(date +%Y%m%d_%H%M%S)

# Supprimer (apr√®s validation que tout fonctionne)
# rm -rf base/
# (On attend validation √©tape 7 avant suppression d√©finitive)
```

#### 5.5 Validation de la Migration

```bash
cd /root/vixens/terraform/environments/dev

# R√©initialiser Terraform avec nouvelle config
terraform init -upgrade

# Valider la syntaxe
terraform validate

# Plan - DOIT montrer "No changes" si migration correcte
terraform plan

# Si plan montre des changements de ressources:
# - Analyser les diff√©rences
# - Ajuster le code si n√©cessaire
# - L'objectif est "No changes" (idempotence)
```

### Crit√®res de Succ√®s
- ‚úÖ `terraform validate` r√©ussit
- ‚úÖ `terraform plan` montre "No changes"
- ‚úÖ Aucun appel au module `base/`
- ‚úÖ Architecture √† 2 niveaux (env ‚Üí modules)

### Points de Vigilance
‚ö†Ô∏è **CRITIQUE:**
- `terraform plan` doit absolument montrer "No changes"
- Si des ressources doivent √™tre recr√©√©es, **STOP** et analyser
- Ne pas appliquer si plan montre destruction/recr√©ation

---

## √âTAPE 6: S√âCURISATION BACKEND

### Objectif
Supprimer credentials hardcod√©s du backend S3

### Dur√©e Estim√©e
15 minutes

### Actions D√©taill√©es

#### 6.1 Modifier `environments/dev/backend.tf`

**Fichier:** `environments/dev/backend.tf` (MODIFIER)

```hcl
# ============================================================================
# TERRAFORM BACKEND - S3 Compatible (Minio)
# ============================================================================
# State stock√© sur Minio (S3-compatible) pour collaboration et safety
#
# Configuration des credentials:
# export AWS_ACCESS_KEY_ID="terraform"
# export AWS_SECRET_ACCESS_KEY="terraform"

terraform {
  backend "s3" {
    bucket = "terraform-state-dev"
    key    = "terraform.tfstate"
    region = "us-east-1"

    # Minio endpoint
    endpoint = "http://synelia.internal.truxonline.com:9000"

    # ‚ö†Ô∏è Credentials via variables d'environnement
    # Ne JAMAIS hardcoder access_key et secret_key ici !
    # Configuration requise:
    #   export AWS_ACCESS_KEY_ID="terraform"
    #   export AWS_SECRET_ACCESS_KEY="terraform"

    # S3-compatible settings
    skip_credentials_validation = true
    skip_metadata_api_check     = true
    skip_region_validation      = true
    skip_requesting_account_id  = true
    force_path_style            = true
  }
}
```

#### 6.2 Cr√©er Script de Configuration Environnement

**Fichier:** `terraform/scripts/set-backend-credentials.sh` (NOUVEAU)

```bash
#!/bin/bash
# ============================================================================
# SET BACKEND CREDENTIALS - S3/Minio
# ============================================================================
# Configure les credentials Terraform backend via variables d'environnement
#
# Usage:
#   source ./scripts/set-backend-credentials.sh [environment]
#
# Example:
#   source ./scripts/set-backend-credentials.sh dev

set -euo pipefail

ENVIRONMENT="${1:-dev}"

echo "üîë Configuring Terraform backend credentials for: $ENVIRONMENT"

# ‚ö†Ô∏è TODO: Remplacer par un syst√®me de secrets management
# Options futures:
# - Vault HashiCorp
# - AWS Secrets Manager
# - Azure Key Vault
# - SOPS (Secrets OPerationS)

case "$ENVIRONMENT" in
  dev|test|staging|prod)
    export AWS_ACCESS_KEY_ID="terraform"
    export AWS_SECRET_ACCESS_KEY="terraform"
    export TF_VAR_environment="$ENVIRONMENT"
    echo "‚úÖ Backend credentials configured for $ENVIRONMENT"
    echo "   AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}"
    echo "   Backend endpoint: http://synelia.internal.truxonline.com:9000"
    ;;
  *)
    echo "‚ùå Unknown environment: $ENVIRONMENT"
    echo "   Valid options: dev, test, staging, prod"
    exit 1
    ;;
esac

echo ""
echo "üí° To use:"
echo "   source ./scripts/set-backend-credentials.sh $ENVIRONMENT"
echo "   cd environments/$ENVIRONMENT"
echo "   terraform init"
```

```bash
# Rendre le script ex√©cutable
chmod +x /root/vixens/terraform/scripts/set-backend-credentials.sh
```

#### 6.3 Cr√©er Documentation Backend

**Fichier:** `terraform/README-BACKEND.md` (NOUVEAU)

```markdown
# Terraform Backend Configuration

## Overview

L'infrastructure Vixens utilise un backend S3-compatible (Minio) pour le stockage du state Terraform.

## Configuration

### Credentials

Les credentials ne sont **JAMAIS** hardcod√©s dans les fichiers Terraform.

**M√©thode recommand√©e:**

```bash
# Configurer les variables d'environnement
export AWS_ACCESS_KEY_ID="terraform"
export AWS_SECRET_ACCESS_KEY="terraform"

# Ou utiliser le script helper
source ./scripts/set-backend-credentials.sh dev
```

### Backend per Environment

Chaque environnement a son propre bucket S3:

| Environment | Bucket | Endpoint |
|-------------|--------|----------|
| dev | terraform-state-dev | http://synelia.internal.truxonline.com:9000 |
| test | terraform-state-test | http://synelia.internal.truxonline.com:9000 |
| staging | terraform-state-staging | http://synelia.internal.truxonline.com:9000 |
| prod | terraform-state-prod | http://synelia.internal.truxonline.com:9000 |

## Initialization

```bash
# 1. Configurer credentials
source ./scripts/set-backend-credentials.sh dev

# 2. Naviguer vers l'environnement
cd environments/dev

# 3. Initialiser Terraform
terraform init

# 4. V√©rifier le state
terraform state list
```

## Migration depuis Local State

Si vous migrez depuis un state local:

```bash
# 1. Backup du state local
cp terraform.tfstate terraform.tfstate.backup

# 2. Reconfigurer backend
terraform init -reconfigure

# 3. Valider la migration
terraform state list
```

## Security Best Practices

- ‚úÖ Credentials via variables d'environnement
- ‚úÖ Bucket par environnement (isolation)
- ‚úÖ State locking enabled (si support√© par Minio)
- ‚ö†Ô∏è TODO: Chiffrement at-rest (Minio encryption)
- ‚ö†Ô∏è TODO: Migration vers Vault/SOPS pour credentials

## Troubleshooting

### Error: "credentials not found"

```bash
# Solution: Configurer les variables d'environnement
export AWS_ACCESS_KEY_ID="terraform"
export AWS_SECRET_ACCESS_KEY="terraform"
```

### Error: "bucket does not exist"

```bash
# Cr√©er le bucket sur Minio
# Via UI Minio: http://synelia.internal.truxonline.com:9001
# Ou via mc CLI:
mc mb minio/terraform-state-dev
```
```

#### 6.4 Tester la Configuration

```bash
cd /root/vixens/terraform/environments/dev

# Backup backend actuel
cp backend.tf backend.tf.old

# Modifier backend.tf (supprimer access_key/secret_key hardcod√©s)

# Configurer credentials
export AWS_ACCESS_KEY_ID="terraform"
export AWS_SECRET_ACCESS_KEY="terraform"

# R√©initialiser backend
terraform init -reconfigure

# V√©rifier que le state est accessible
terraform state list

# Plan - doit fonctionner
terraform plan
```

### Crit√®res de Succ√®s
- ‚úÖ Aucun credential hardcod√© dans `backend.tf`
- ‚úÖ Script helper cr√©√© et fonctionnel
- ‚úÖ Documentation backend compl√®te
- ‚úÖ `terraform init` fonctionne avec env vars

### Points de Vigilance
‚ö†Ô∏è **Important:**
- Ne JAMAIS commiter de credentials dans Git
- Ajouter `.env` au `.gitignore` si utilis√©
- Tester avec `terraform state list` apr√®s migration

---

## √âTAPE 7: VALIDATION FINALE

### Objectif
Valider que le refactoring est complet, idempotent et fonctionnel

### Dur√©e Estim√©e
45 minutes

### Actions D√©taill√©es

#### 7.1 Validation Syntaxique

```bash
cd /root/vixens/terraform

# Format r√©cursif
terraform fmt -recursive

# V√©rifier qu'aucun changement
git diff

# Validation de tous les environnements
for env in dev test staging prod; do
  echo "=== Validating $env ==="
  cd environments/$env
  terraform init -backend=false
  terraform validate
  cd ../..
done
```

#### 7.2 Validation des Modules

```bash
cd /root/vixens/terraform/modules

# Valider chaque module ind√©pendamment
for module in shared talos cilium argocd; do
  echo "=== Validating module: $module ==="
  cd $module
  terraform init -backend=false
  terraform validate
  cd ..
done
```

#### 7.3 Test Idempotence (CRITIQUE)

```bash
cd /root/vixens/terraform/environments/dev

# Configurer backend
source ../../scripts/set-backend-credentials.sh dev

# Init
terraform init

# Plan - DOIT montrer "No changes"
terraform plan -detailed-exitcode

# Exit code:
# 0 = Success, no changes
# 1 = Error
# 2 = Success, changes present

# On veut exit code = 0
if [ $? -eq 0 ]; then
  echo "‚úÖ IDEMPOTENT: No changes detected"
else
  echo "‚ùå FAILED: Plan shows changes or errors"
  terraform plan
  exit 1
fi
```

#### 7.4 Test Destroy/Recreate (Ultime Validation)

**‚ö†Ô∏è ATTENTION: Ceci d√©truit l'infrastructure dev !**

```bash
cd /root/vixens/terraform/environments/dev

# 1. Backup final du state
terraform state pull > state.pre-destroy.$(date +%Y%m%d_%H%M%S).json

# 2. Documenter l'√©tat actuel
terraform output -json > outputs.pre-destroy.json
kubectl --kubeconfig=kubeconfig-dev get nodes -o wide > nodes.pre-destroy.txt
kubectl --kubeconfig=kubeconfig-dev get pods -A > pods.pre-destroy.txt

# 3. Destroy
echo "üî• Destroying dev infrastructure..."
terraform destroy -auto-approve

# 4. Recreate
echo "üî® Recreating dev infrastructure..."
terraform apply -auto-approve

# 5. Validation post-recreate
echo "‚úÖ Validating recreated infrastructure..."

# Attendre que le cluster soit pr√™t
sleep 60

# V√©rifier nodes
kubectl --kubeconfig=kubeconfig-dev get nodes

# V√©rifier pods syst√®me
kubectl --kubeconfig=kubeconfig-dev get pods -n kube-system
kubectl --kubeconfig=kubeconfig-dev get pods -n argocd

# V√©rifier ArgoCD
curl -I http://192.168.208.71

# 6. Final plan (doit √™tre "No changes")
terraform plan -detailed-exitcode
```

#### 7.5 M√©triques de R√©ussite

**Fichier:** `terraform/REFACTORING-METRICS.md` (NOUVEAU)

```markdown
# Refactoring Metrics - Terraform Infrastructure

**Date:** $(date +%Y-%m-%d)
**Status:** ‚úÖ COMPLETED

## Code Metrics

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Total lines | ~1400 | ~900 | -35% |
| Files | 41 | 28 | -32% |
| Variables (env) | 27 flat | 8 objects | -70% |
| Typed variables | 0% | 100% | +100% |
| Architecture levels | 3 | 2 | -33% |
| Duplicated code | ArgoCD√ó2, tolerations√ó15 | 0 | -100% |

## Technical Debt Eliminated

- ‚úÖ Removed duplicate ArgoCD code (base/ vs modules/)
- ‚úÖ Eliminated 15+ toleration duplications
- ‚úÖ Centralized chart versions (Cilium, ArgoCD)
- ‚úÖ Removed 3-tier architecture (base/ wrapper)
- ‚úÖ Typed all variables with validation
- ‚úÖ Secured backend (no hardcoded credentials)
- ‚úÖ Dynamic paths (no fragile relative paths)

## DRY Principle Applied

### Before
- Tolerations: 15 repetitions
- ArgoCD code: 2 copies (180 lines each)
- Variables: 27√ó4 = 108 pass-through declarations
- Versions: Hardcoded 3 times

### After
- Tolerations: 1 definition in `modules/shared`
- ArgoCD code: 1 optimized module
- Variables: 8 objects in `environments/*/variables.tf`
- Versions: Centralized in `modules/shared/locals.tf`

## Validation Results

- ‚úÖ `terraform validate`: PASS (all envs)
- ‚úÖ `terraform plan`: No changes (idempotent)
- ‚úÖ Destroy/Recreate test: SUCCESS
- ‚úÖ Cluster functional: 3 nodes HA
- ‚úÖ ArgoCD accessible: http://192.168.208.71
- ‚úÖ Cilium operational: CNI + LB IPAM

## Best Practices Compliance

- ‚úÖ Strong typing (100% variables typed)
- ‚úÖ Validation rules (IP, CIDR, versions)
- ‚úÖ DRY principle (zero duplication)
- ‚úÖ Separation of concerns (focused modules)
- ‚úÖ Security (credentials via env vars)
- ‚úÖ Documentation (inline + dedicated files)

## Maintainability Score

**Before:** 3/10
**After:** 8/10
**Improvement:** +166%

Factors:
- Code clarity: 4/10 ‚Üí 9/10
- Reusability: 2/10 ‚Üí 9/10
- Documentation: 5/10 ‚Üí 8/10
- Security: 2/10 ‚Üí 7/10
```

#### 7.6 Update Documentation

```bash
# Mettre √† jour CLAUDE.md avec nouvelle architecture
cd /root/vixens

# Section √† modifier:
# - Repository Structure (supprimer terraform/base/)
# - Terraform Module interface (ajouter module shared)
# - Development Commands (ajouter script backend)
```

**Modifications CLAUDE.md:**

Ajouter apr√®s "Repository Structure":

```markdown
### Terraform Module: shared (NEW - DRY Configurations)

Location: `terraform/modules/shared/`

**Centralized configurations to eliminate code duplication:**

- Chart versions (Cilium, ArgoCD)
- Kubernetes tolerations (control-plane)
- Cilium annotations and capabilities
- Environment-specific configurations
- Common labels and networking constants

**Key Features:**
- ‚úÖ Zero duplication (DRY principle)
- ‚úÖ Single source of truth for versions
- ‚úÖ Environment-aware configurations
- ‚úÖ Reusable across all modules

**Exports:**
```hcl
module.shared.cilium_version              # "1.18.3"
module.shared.argocd_version              # "7.7.7"
module.shared.control_plane_tolerations   # Standard tolerations
module.shared.cilium_agent_capabilities   # Linux capabilities
module.shared.common_labels               # Kubernetes labels
```
```

#### 7.7 Commit & PR

```bash
cd /root/vixens

# V√©rifier les changements
git status
git diff

# Ajouter les fichiers
git add terraform/
git add docs/terraform-refactoring-implementation-plan.md
git add docs/REFACTORING-METRICS.md

# Supprimer base/ (si validation OK)
git rm -r terraform/base/

# Commit
git commit -m "refactor(terraform): DRY optimization and best practices

BREAKING CHANGE: Architecture refactored from 3 to 2 levels

**Major Changes:**
- Remove terraform/base/ wrapper (3-tier ‚Üí 2-tier architecture)
- Add modules/shared/ for DRY configurations
- Restructure 27 flat variables ‚Üí 8 typed objects
- Eliminate ArgoCD duplication (base/ vs modules/)
- Centralize tolerations, versions, capabilities
- Secure backend (credentials via env vars)

**Metrics:**
- Code: -35% (1400 ‚Üí 900 lines)
- Variables: -70% (27 ‚Üí 8 objects)
- Duplication: -100% (zero repetition)
- Typed variables: +100% (0% ‚Üí 100%)

**Validation:**
- ‚úÖ terraform validate: PASS
- ‚úÖ terraform plan: No changes (idempotent)
- ‚úÖ Destroy/recreate test: SUCCESS
- ‚úÖ Cluster operational: 3 nodes HA

**Files Changed:**
- Added: modules/shared/ (DRY configurations)
- Modified: environments/*/main.tf (direct module calls)
- Modified: environments/*/variables.tf (typed objects)
- Modified: modules/argocd/ (optimized, no duplication)
- Modified: modules/cilium/ (DRY via shared)
- Removed: terraform/base/ (eliminated wrapper)

Refs: docs/terraform-refactoring-implementation-plan.md

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"

# Push
git push origin terraform-refactor

# Cr√©er PR (si gh CLI disponible)
gh pr create \
  --title "refactor(terraform): DRY optimization and best practices" \
  --body "$(cat <<'EOF'
## Summary

Refactoring complet de l'infrastructure Terraform selon les best practices et le principe DRY.

## Changes

### Architecture
- ‚ùå Suppression `terraform/base/` (wrapper inutile)
- ‚úÖ Architecture 2 niveaux: `environments/` ‚Üí `modules/`
- ‚úÖ Nouveau module `shared/` pour configurations DRY

### Variables
- ‚úÖ 27 variables plates ‚Üí 8 objets typ√©s
- ‚úÖ 100% des variables avec type + validation
- ‚úÖ Groupement logique (cluster, argocd, cilium_l2, etc.)

### Code Quality
- ‚úÖ √âlimination duplication ArgoCD (base/ vs modules/)
- ‚úÖ Centralisation tolerations (15 r√©p√©titions ‚Üí 1 d√©finition)
- ‚úÖ Versions centralis√©es (Cilium, ArgoCD)
- ‚úÖ Paths dynamiques (plus de paths relatifs fragiles)

### Security
- ‚úÖ Backend s√©curis√© (credentials via env vars)
- ‚úÖ Script helper pour configuration

## Metrics

| Metric | Before | After | Gain |
|--------|--------|-------|------|
| Lines | 1400 | 900 | -35% |
| Variables | 27 | 8 | -70% |
| Duplication | Many | 0 | -100% |
| Typed vars | 0% | 100% | +100% |

## Validation

- ‚úÖ `terraform validate` sur tous les environnements
- ‚úÖ `terraform plan` montre "No changes" (idempotent)
- ‚úÖ Test destroy/recreate r√©ussi sur dev
- ‚úÖ Cluster op√©rationnel (3 nodes HA)

## Breaking Changes

‚ö†Ô∏è Architecture change: `terraform/base/` supprim√©

**Migration requise:**
1. Update imports: `../../base` ‚Üí `../../modules/{talos,cilium,argocd}`
2. Update variables: flat vars ‚Üí objects
3. Reconfigure backend: `source scripts/set-backend-credentials.sh`

**Rollback:** Tag `pre-refactor-$(date +%Y%m%d)` disponible

## Documentation

- üìÑ Plan d'impl√©mentation: `docs/terraform-refactoring-implementation-plan.md`
- üìä M√©triques: `docs/REFACTORING-METRICS.md`
- üìù Backend: `terraform/README-BACKEND.md`

## Testing

```bash
# Test rapide
cd terraform/environments/dev
source ../../scripts/set-backend-credentials.sh dev
terraform init
terraform plan # Should show "No changes"
```

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
EOF
)" \
  --base dev
```

### Crit√®res de Succ√®s
- ‚úÖ Tous les tests passent (validate, plan, destroy/recreate)
- ‚úÖ Documentation mise √† jour
- ‚úÖ M√©triques document√©es
- ‚úÖ PR cr√©√©e avec description compl√®te
- ‚úÖ `terraform plan` = "No changes" (idempotent)

---

## ROLLBACK PROCEDURE

### En Cas de Probl√®me

#### Option 1: Rollback Git

```bash
# Revenir au tag pr√©-refactor
cd /root/vixens
git checkout pre-refactor-$(date +%Y%m%d)

# Ou annuler commits
git reset --hard HEAD~1
git push --force origin terraform-refactor
```

#### Option 2: Restaurer State

```bash
cd /root/vixens/terraform/environments/dev

# Lister les backups
ls -lah terraform.tfstate.backup.*

# Restaurer le dernier backup
cp terraform.tfstate.backup.YYYYMMDD_HHMMSS terraform.tfstate

# Ou restaurer depuis remote
terraform state pull > current.tfstate
# Modifier manuellement si n√©cessaire
terraform state push current.tfstate
```

#### Option 3: Rollback Partiel

Si seulement certaines √©tapes posent probl√®me:

```bash
# Restaurer fichiers sp√©cifiques
git checkout HEAD~1 -- terraform/environments/dev/main.tf
git checkout HEAD~1 -- terraform/environments/dev/variables.tf

# R√©initialiser
terraform init -reconfigure
terraform plan
```

### Points de Non-Retour

‚ö†Ô∏è **STOP le refactoring si:**

1. `terraform plan` montre destruction de ressources
2. State Terraform corrompu
3. Cluster inaccessible apr√®s changements
4. Tests destroy/recreate √©chouent

**Action:** Revenir au tag `pre-refactor-YYYYMMDD` imm√©diatement

---

## CHECKLIST FINALE

### Avant de Merger la PR

- [ ] `terraform fmt -recursive` sans changements
- [ ] `terraform validate` sur dev, test, staging, prod
- [ ] `terraform plan` sur dev = "No changes"
- [ ] Test destroy/recreate r√©ussi
- [ ] Backend s√©curis√© (pas de credentials hardcod√©s)
- [ ] Documentation √† jour (CLAUDE.md, README-BACKEND.md)
- [ ] M√©triques document√©es (REFACTORING-METRICS.md)
- [ ] Aucune duplication de code
- [ ] 100% variables typ√©es
- [ ] Module `shared/` fonctionnel
- [ ] R√©pertoire `base/` supprim√©
- [ ] Script backend credentials fonctionnel
- [ ] PR description compl√®te
- [ ] Tests CI/CD passent (si configur√©)
- [ ] Review par pair (si applicable)

### Post-Merge

- [ ] Merger PR vers `dev`
- [ ] Tester sur environnement test
- [ ] Valider pendant 1 semaine
- [ ] Merger vers `main` (production)
- [ ] Supprimer branche `terraform-refactor`
- [ ] Supprimer backups (apr√®s 30 jours)
- [ ] C√©l√©brer le refactoring r√©ussi üéâ

---

## TIMELINE ESTIM√âE

| √âtape | Dur√©e | Cumul√© |
|-------|-------|--------|
| 0. Pr√©paration | 15 min | 15 min |
| 1. Module Shared | 30 min | 45 min |
| 2. Typage Variables | 45 min | 1h30 |
| 3. Fusion ArgoCD | 40 min | 2h10 |
| 4. Optimisation Cilium | 30 min | 2h40 |
| 5. Suppression Base | 60 min | 3h40 |
| 6. S√©curisation Backend | 15 min | 3h55 |
| 7. Validation Finale | 45 min | 4h40 |

**Total estim√©:** 4h40 (sans interruptions)
**Total r√©aliste:** 6-8h (avec tests, debugging, documentation)

---

## SUPPORT & QUESTIONS

### Ressources

- **Documentation Terraform:** https://www.terraform.io/docs
- **Best Practices:** https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html
- **Talos Provider:** https://registry.terraform.io/providers/siderolabs/talos
- **Ce plan:** `/root/vixens/docs/terraform-refactoring-implementation-plan.md`

### Contact

En cas de probl√®me pendant l'impl√©mentation:

1. Consulter la section [Rollback Procedure](#rollback-procedure)
2. V√©rifier les logs Terraform (`terraform plan -out=debug.tfplan`)
3. Valider le state (`terraform state list`)
4. Revenir au tag `pre-refactor-YYYYMMDD` si bloqu√©

---

**END OF IMPLEMENTATION PLAN**
