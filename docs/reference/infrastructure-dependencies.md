# Infrastructure Dependencies

**Last Updated:** 2026-02-07
**Status:** Active

---

## ğŸ¯ Purpose

This document maps the infrastructure dependencies in the Vixens Kubernetes cluster, with particular focus on critical paths that can cause cascade failures. Understanding these dependencies is essential for:

- **Incident Response:** Quickly identify root cause of failures
- **Change Management:** Assess blast radius before making changes
- **Capacity Planning:** Understand resource constraints
- **DR Planning:** Prioritize recovery order

---

## ğŸ“Š Dependency Hierarchy

### Level 0: Foundation (Hardware/Network)

```
Physical Infrastructure
â”œâ”€â”€ Synology NAS (192.168.111.69)
â”‚   â”œâ”€â”€ iSCSI Target Service
â”‚   â”œâ”€â”€ NFS Service
â”‚   â””â”€â”€ Infisical (http://192.168.111.69:8085)
â”œâ”€â”€ Network Infrastructure
â”‚   â”œâ”€â”€ VLAN 111 (Internal) - 192.168.111.0/24
â”‚   â””â”€â”€ VLAN 20X (Services) - 192.168.20X.0/24
â””â”€â”€ Talos Nodes
    â”œâ”€â”€ Control Planes (3x HA)
    â””â”€â”€ Worker Nodes
```

**Critical Failure Impact:** Complete cluster outage

---

### Level 1: Core Infrastructure Services

```
Kubernetes Core
â”œâ”€â”€ etcd (Control Plane)
â”œâ”€â”€ kube-apiserver
â”œâ”€â”€ kube-controller-manager
â”œâ”€â”€ kube-scheduler
â””â”€â”€ kubelet (all nodes)

CNI (Cilium)
â”œâ”€â”€ cilium-operator
â”œâ”€â”€ cilium-agent (DaemonSet)
â””â”€â”€ Cilium L2 IPAM + LB

Ingress Controller (Traefik)
â”œâ”€â”€ traefik-controller
â””â”€â”€ LoadBalancer Service (192.168.201.70)

Storage Drivers
â”œâ”€â”€ Synology CSI Driver âš ï¸ CRITICAL
â”‚   â”œâ”€â”€ synology-csi-controller
â”‚   â””â”€â”€ synology-csi-node (DaemonSet)
â””â”€â”€ NFS Provisioner
    â””â”€â”€ nfs-subdir-external-provisioner
```

**Critical Failure Impact:** Cannot schedule pods, no network, no storage, no ingress

---

### Level 2: Platform Services

```
Secret Management
â””â”€â”€ Infisical Operator âš ï¸ CRITICAL
    â”œâ”€â”€ infisical-controller-manager
    â””â”€â”€ Universal Auth Secret (namespace: argocd)

GitOps (ArgoCD)
â”œâ”€â”€ argocd-server
â”œâ”€â”€ argocd-application-controller
â”œâ”€â”€ argocd-repo-server
â””â”€â”€ argocd-redis

Policy Enforcement
â””â”€â”€ Kyverno âš ï¸ CRITICAL (Webhook)
    â”œâ”€â”€ kyverno-admission-controller
    â”œâ”€â”€ kyverno-background-controller
    â”œâ”€â”€ kyverno-cleanup-controller
    â””â”€â”€ kyverno-reports-controller

Monitoring
â”œâ”€â”€ Prometheus
â”œâ”€â”€ Grafana
â”œâ”€â”€ Loki
â””â”€â”€ Promtail
```

**Critical Failure Impact:**
- Infisical: Cannot sync secrets â†’ Apps fail
- ArgoCD: Cannot deploy/sync apps
- Kyverno: Cannot validate resources â†’ Sync blocked
- Monitoring: Blind operations (no impact on apps)

---

### Level 3: Shared Services (Databases)

```
PostgreSQL (CloudNativePG)
â””â”€â”€ postgresql-shared âš ï¸ CRITICAL
    â”œâ”€â”€ Uses: iSCSI PVC (RWO)
    â””â”€â”€ Dependents: 10+ applications

MariaDB
â””â”€â”€ mariadb-shared âš ï¸ CRITICAL
    â”œâ”€â”€ Uses: iSCSI PVC (RWO)
    â””â”€â”€ Dependents: 5+ applications

Redis
â””â”€â”€ redis-shared
    â””â”€â”€ Dependents: Various caching needs
```

**Critical Failure Impact:** All dependent applications fail

---

### Level 4: Application Services

Applications are categorized by their storage dependencies:

#### Category A: iSCSI-dependent (HIGH RISK)

These apps use ReadWriteOnce (RWO) volumes from Synology CSI:

```
Home Automation
â”œâ”€â”€ Home Assistant (iSCSI + strategy: Recreate)
â”œâ”€â”€ Mosquitto (StatefulSet, iSCSI)
â””â”€â”€ Music Assistant

Media Stack
â”œâ”€â”€ Jellyfin (iSCSI)
â”œâ”€â”€ Prowlarr (iSCSI)
â”œâ”€â”€ Radarr (iSCSI)
â”œâ”€â”€ Sonarr (iSCSI)
â”œâ”€â”€ Lidarr (iSCSI)
â”œâ”€â”€ LazyLibrarian (iSCSI)
â”œâ”€â”€ Whisparr (iSCSI)
â”œâ”€â”€ qBittorrent (iSCSI)
â”œâ”€â”€ SABnzbd (iSCSI)
â””â”€â”€ Pyload (iSCSI)

Tools
â”œâ”€â”€ Penpot (iSCSI)
â”œâ”€â”€ NocoDB (iSCSI)
â”œâ”€â”€ Vikunja (iSCSI)
â”œâ”€â”€ Linkwarden (iSCSI)
â””â”€â”€ Stirling-PDF (iSCSI)

Networking
â”œâ”€â”€ AdGuard Home (iSCSI + Litestream)
â””â”€â”€ NetBird (iSCSI)
```

**Failure Cascade Path:**
```
CSI credentials invalid
    â†“
iSCSI login failures
    â†“
Volume attach/detach errors
    â†“
Multi-Attach errors
    â†“
Pods stuck ContainerCreating
    â†“
Applications Degraded/Progressing
```

#### Category B: Database-dependent (MEDIUM RISK)

These apps depend on shared PostgreSQL/MariaDB:

```
PostgreSQL-dependent
â”œâ”€â”€ Authentik (auth + postgresql-shared)
â”œâ”€â”€ Mealie (postgresql-shared)
â”œâ”€â”€ Firefly III (postgresql-shared)
â”œâ”€â”€ NetBox (postgresql-shared)
â”œâ”€â”€ Docspell (postgresql-shared)
â””â”€â”€ Contacts (postgresql-shared)

MariaDB-dependent
â”œâ”€â”€ Booklore (mariadb-shared)
â”œâ”€â”€ Vikunja (mariadb-shared)
â””â”€â”€ [others]
```

**Failure Cascade Path:**
```
CSI credentials invalid
    â†“
Database PVC cannot mount
    â†“
Database pods stuck Init
    â†“
Dependent apps cannot connect
    â†“
Applications Progressing/Degraded
```

#### Category C: NFS-dependent (LOW RISK)

These apps use NFS from Synology (no authentication needed):

```
â”œâ”€â”€ Velero (backup storage)
â”œâ”€â”€ Media shared storage (NFS)
â””â”€â”€ Various apps with NFS volumes
```

**Failure Impact:** Minimal, NFS more resilient than iSCSI

#### Category D: Stateless (NO RISK)

These apps have no persistent storage:

```
â”œâ”€â”€ whoami
â”œâ”€â”€ IT-Tools
â”œâ”€â”€ Headlamp
â””â”€â”€ Various monitoring components
```

**Failure Impact:** None, can restart anywhere

---

## ğŸ”¥ Critical Failure Scenarios

### Scenario 1: CSI Credentials Invalid (2026-02-07 Incident)

**Trigger:** Synology DSM password changed without updating Infisical secret

**Cascade Path:**
```
DSM password change
    â†“
synology-csi-credentials-sync (Infisical) outdated
    â†“
CSI driver authentication failures
    â†“
iSCSI login errors: "Failed to login with target iqn"
    â†“
Volume attachment failures + Multi-Attach errors
    â†“
Pods stuck: ContainerCreating, Init:0/1, Pending
    â†“
Databases cannot start (postgresql-shared, mariadb-shared)
    â†“
Database-dependent apps fail (authentik, mealie, etc.)
    â†“
Resource contention from cascading restarts
    â†“
Kyverno webhook temporarily unavailable (collateral)
    â†“
ArgoCD sync failures â†’ apps OutOfSync
    â†“
Cluster-wide degradation
```

**Affected:** 50+ applications, 2+ hours recovery time

**Prevention:** Follow [DSM Password Change Procedure](../procedures/dsm-password-change.md)

---

### Scenario 2: Infisical Operator Failure

**Trigger:** Infisical operator crashes or loses connection to Infisical server

**Cascade Path:**
```
Infisical operator down
    â†“
InfisicalSecrets not synced
    â†“
Kubernetes secrets stale/missing
    â†“
Apps using these secrets fail (CSI, cert-manager, external-dns, etc.)
    â†“
Cascading failures based on affected secrets
```

**Affected:** All apps using InfisicalSecret

**Mitigation:**
- Existing secrets persist (not deleted)
- Manual secret creation possible as emergency workaround

---

### Scenario 3: Kyverno Webhook Unavailable

**Trigger:** Kyverno pods restart during cluster disruption

**Cascade Path:**
```
Kyverno admission-controller unavailable
    â†“
Webhook validation failures
    â†“
ArgoCD cannot apply resources
    â†“
Applications stuck OutOfSync
    â†“
Recovery blocked until Kyverno stabilizes
```

**Affected:** All ArgoCD applications during incident window

**Mitigation:**
- Kyverno has FailurePolicy: Fail (safe default)
- Wait for Kyverno to recover (usually <5 min)
- Temporary: Scale down Kyverno (emergency only!)

---

### Scenario 4: Shared Database Corruption

**Trigger:** I/O errors, node crash, improper shutdown

**Cascade Path:**
```
Database corruption (PostgreSQL/MariaDB)
    â†“
Pod CrashLoopBackOff or Init:0/1
    â†“
All dependent apps cannot connect
    â†“
Apps Degraded (waiting for DB)
```

**Affected:** 10-15 applications per database

**Recovery:**
1. Delete pod (force reschedule)
2. If PVC corrupted: Restore from Velero backup
3. If I/O error: Check Synology NAS health

---

## ğŸ“ˆ Dependency Impact Matrix

| Component | Direct Dependents | Indirect Dependents | MTTR | Blast Radius |
|-----------|-------------------|---------------------|------|--------------|
| **Synology CSI** | 40+ apps (iSCSI PVCs) | All cluster (critical) | 30-60 min | ğŸ”´ CRITICAL |
| **Infisical Operator** | 20+ apps (secrets) | Dependent apps | 10-30 min | ğŸŸ  HIGH |
| **postgresql-shared** | 10+ apps | None | 5-15 min | ğŸŸ¡ MEDIUM |
| **mariadb-shared** | 5+ apps | None | 5-15 min | ğŸŸ¡ MEDIUM |
| **Kyverno** | ArgoCD (webhook) | All apps (indirect) | 2-5 min | ğŸŸ  HIGH |
| **ArgoCD** | Deployment workflow | None (apps keep running) | 10-20 min | ğŸŸ¢ LOW |
| **Traefik** | Ingress access | None (pods run) | 5-10 min | ğŸŸ¢ LOW |

**MTTR:** Mean Time To Recovery (estimated)
**Blast Radius:** Scope of impact on cluster

---

## ğŸ›¡ï¸ Best Practices

### 1. Change Management

**ALWAYS assess dependencies before making changes:**

```bash
# Before changing DSM password
1. Check CSI dependency: kubectl get pods -n synology-csi
2. List affected PVCs: kubectl get pvc -A | grep synology
3. Review procedure: docs/procedures/dsm-password-change.md
4. Plan maintenance window: 30-60 min

# Before upgrading shared database
1. List dependent apps: See Level 4 dependency tree
2. Test upgrade in dev first
3. Have rollback plan (Velero backup)
4. Communicate downtime window
```

### 2. Incident Response

**Follow dependency tree from bottom-up:**

```
1. Identify failing apps (Level 4)
2. Check databases (Level 3)
3. Verify platform services (Level 2)
4. Validate core infrastructure (Level 1)
5. Inspect foundation (Level 0)
```

**Use dependency knowledge to narrow investigation:**
- Multiple apps failing â†’ Check shared database
- All apps with PVCs failing â†’ Check CSI driver
- ArgoCD sync failures â†’ Check Kyverno webhook
- No new pods scheduling â†’ Check core Kubernetes

### 3. Monitoring Priorities

**Alert on these critical paths:**

```yaml
Critical Alerts (P0):
  - Synology CSI authentication failures
  - Kyverno webhook unavailable
  - Infisical operator down
  - etcd unhealthy
  - Control plane node down

High Alerts (P1):
  - Shared database unhealthy
  - ArgoCD sync failures (>5 apps)
  - Traefik LoadBalancer down

Medium Alerts (P2):
  - Individual app failures
  - PVC mount delays
  - Resource limits reached
```

### 4. Testing Resilience

**Chaos engineering scenarios:**

```bash
# Test CSI failure recovery
1. Intentionally break CSI credentials
2. Observe cascade
3. Follow recovery procedure
4. Document actual vs. expected behavior

# Test database failure recovery
1. Delete database pod
2. Verify dependent apps handle gracefully
3. Confirm auto-recovery

# Test Kyverno webhook failure
1. Scale Kyverno to 0 temporarily
2. Attempt ArgoCD sync
3. Verify FailurePolicy behavior
4. Scale back and confirm recovery
```

---

## ğŸ” Troubleshooting Checklist

When investigating failures, follow this systematic approach:

### Step 1: Identify Symptoms
```bash
# Applications unhealthy
kubectl get applications -n argocd | grep -v "Synced.*Healthy"

# Pods not running
kubectl get pods -A --field-selector status.phase!=Running,status.phase!=Succeeded

# PVC issues
kubectl get pvc -A | grep -v Bound

# Volume attachment problems
kubectl get volumeattachments | grep -v true
```

### Step 2: Check Critical Components
```bash
# CSI Driver
kubectl get pods -n synology-csi
kubectl logs -n synology-csi synology-csi-controller-0 -c synology-csi-plugin --tail=50

# Infisical Operator
kubectl get pods -n infisical-operator-system
kubectl get infisicalsecret -A

# Kyverno
kubectl get pods -n kyverno
kubectl logs -n kyverno -l app.kubernetes.io/component=admission-controller --tail=50

# Databases
kubectl get pods -n databases
```

### Step 3: Review Events
```bash
# Recent cluster events
kubectl get events -A --sort-by='.lastTimestamp' | tail -50

# Namespace-specific events
kubectl get events -n {namespace} --sort-by='.lastTimestamp'
```

### Step 4: Consult Runbooks
- **CSI issues:** [DSM Password Change Procedure](../procedures/dsm-password-change.md)
- **Cascade failures:** [Cascade Failure Recovery Runbook](../troubleshooting/cascade-failure-recovery.md)
- **Database issues:** Check Level 3 dependency tree above

---

## ğŸ“š Related Documentation

- **[DSM Password Change Procedure](../procedures/dsm-password-change.md)** - Step-by-step for credential updates
- **[Cascade Failure Recovery](../troubleshooting/cascade-failure-recovery.md)** - Generic recovery runbook
- **[Post-Mortem 2026-02-07](../troubleshooting/post-mortems/2026-02-07-dsm-password-cascade-failure.md)** - Real incident analysis
- **[Synology CSI Documentation](../applications/01-storage/synology-csi.md)** - CSI driver details
- **[Application Deployment Standard](application-deployment-standard.md)** - Deployment best practices

---

**Maintained by:** Infrastructure Team
**Review Frequency:** After each major incident or architecture change
