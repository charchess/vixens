# Roadmap Vixens - Infrastructure Kubernetes GitOps

## Vision Globale

Construire une plateforme Kubernetes multi-cluster (dev/test/staging/prod) g√©r√©e en GitOps, suivant les best practices cloud-native, avec approche it√©rative et destruction/reconstruction simplifi√©e.

---

## Phases du Projet

### üì¶ Phase 1 : Infrastructure as Code (Terraform)
**Scope** : Provisioning automatis√© des clusters Talos via Terraform

- Configuration nodes Talos (dual-VLAN)
- Bootstrap Kubernetes
- D√©ploiement CNI (Cilium)
- D√©ploiement ArgoCD
- Validation end-to-end

**Deliverable** : `terraform apply` ‚Üí cluster fonctionnel avec ArgoCD

---

### üöÄ Phase 2 : GitOps Core Services
**Scope** : Services infrastructure g√©r√©s par ArgoCD

- MetalLB (LoadBalancer)
- Traefik (Ingress Controller)
- cert-manager (TLS/HTTPS)
- Synology CSI Driver (iSCSI storage)
- Authelia (SSO/Authentication)
- Monitoring (Prometheus/Grafana)

**Deliverable** : Stack infrastructure compl√®te en GitOps

---

### üéØ Phase 3 : Applications & Utilisation (Hors Scope Initial)
**Scope** : D√©ploiement applications utilisateur

- Media apps (Radarr, Sonarr, Lidarr)
- Downloads (SABnzbd)
- Home automation (Home Assistant)
- Password manager (Vaultwarden)
- NFS storage (bas priorit√©)

**Note** : Cette phase fera l'objet d'un projet s√©par√©

---

## Approche Sprint

### Principes

1. **It√©ratif** : Chaque sprint livre une fonctionnalit√© validable
2. **Incr√©mental** : Partir d'1 node ‚Üí 3 nodes ‚Üí multi-cluster
3. **Destructible** : Clusters dev/test peuvent √™tre reconstruits √† tout moment
4. **Validable** : Chaque sprint a des crit√®res d'acceptation clairs

### Strat√©gie de Validation

**Par Sprint** :
- Tests manuels (commandes document√©es)
- Validation terraform (plan = no changes)
- Validation ArgoCD (all apps synced & healthy)

**Promotion entre Environnements** :
```
DEV (branche dev)
  ‚îî‚îÄ> Validation compl√®te
      ‚îî‚îÄ> PR dev ‚Üí test
          ‚îî‚îÄ> TEST (branche test)
              ‚îî‚îÄ> Validation compl√®te
                  ‚îî‚îÄ> PR test ‚Üí staging
                      ‚îî‚îÄ> STAGING (branche staging)
                          ‚îî‚îÄ> Validation compl√®te
                              ‚îî‚îÄ> PR staging ‚Üí main
                                  ‚îî‚îÄ> PROD (branche main)
```

---

## Phase 1 - D√©tail des Sprints

### Sprint 0 : Pr√©paration (1-2h)
**Objectif** : Structure projet et documentation

**Livrables** :
- ‚úÖ Architecture r√©seau document√©e
- ‚úÖ ADRs (Architecture Decision Records)
- ‚úÖ Structure Git initialis√©e
- ‚úÖ Projet Archon cr√©√©

---

### Sprint 1 : Terraform Module Talos (4-6h)
**Objectif** : Cluster dev √† 1 node (obsy) fonctionnel

**Statut** : ‚úÖ Termin√©

**T√¢ches** :
1. Cr√©er module `terraform/modules/talos/`
   - Variables validation (control planes impair)
   - Configuration dual-VLAN
   - Bootstrap automatique
2. Configurer `terraform/environments/dev/` pour 1 node
3. `terraform apply` ‚Üí node provisionn√©
4. Valider Kubernetes API accessible
5. Valider node Ready

**Validation** :
```bash
talosctl --nodes 192.168.111.162 version
kubectl --kubeconfig kubeconfig-dev get nodes
# R√©sultat attendu : 1 node Ready
```

**Definition of Done** :
- [ ] Module Terraform valid√© (fmt, validate)
- [ ] Cluster 1 node accessible
- [ ] Kubeconfig fonctionnel
- [ ] Documentation module compl√®te

---

### Sprint 2 : D√©ploiement Cilium (2-3h)
**Objectif** : CNI op√©rationnel sur cluster 1 node

**Statut** : ‚úÖ Termin√©

**T√¢ches** :
1. Configurer Helm provider Terraform
2. D√©ployer Cilium via Terraform
   - kube-proxy replacement
   - Hubble enabled
3. Valider pods Cilium running
4. Valider connectivit√© r√©seau

**Validation** :
```bash
kubectl get pods -n kube-system -l k8s-app=cilium
cilium status
cilium connectivity test
```

**Definition of Done** :
- [ ] Cilium pods running (DaemonSet sur 1 node)
- [ ] `cilium connectivity test` = success
- [ ] Hubble relay accessible

---

### Sprint 3 : Scale √† 3 Nodes Dev (3-4h)
**Objectif** : Cluster HA 3 control planes

**Statut** : ‚úÖ Termin√©

**T√¢ches** :
1. Ajouter onyx, opale dans `terraform.tfvars`
2. `terraform plan` ‚Üí v√©rifier 2 nouveaux nodes
3. `terraform apply`
4. Valider 3 nodes Ready
5. Valider etcd quorum (3 membres)
6. Tester cas erreur (variable validation control planes impair)

**Validation** :
```bash
kubectl get nodes
# R√©sultat attendu : 3 nodes Ready

talosctl --nodes 192.168.111.160 etcd members
# R√©sultat attendu : 3 membres
```

**Definition of Done** :
- [ ] 3 nodes Ready
- [ ] etcd quorum fonctionnel
- [ ] Cilium distribu√© sur 3 nodes
- [ ] Validation variable Terraform (nombre impair)

---

### Sprint 4 : ArgoCD Bootstrap (3-4h)
**Objectif** : ArgoCD auto-g√©r√© via GitOps

**Statut** : ‚úÖ Termin√©

**T√¢ches** :
1. D√©ployer ArgoCD via Terraform (Helm chart)
2. Cr√©er structure `argocd/base/` + `overlays/dev/`
3. Cr√©er root-app (App-of-Apps)
4. Commit + push branche `dev`
5. Appliquer root-app manuellement
6. Valider ArgoCD self-managed

**Validation** :
```bash
kubectl get pods -n argocd
argocd app list
# R√©sultat attendu : argocd app synced & healthy
```

**Definition of Done** :
- [ ] ArgoCD UI accessible
- [ ] Root app sync automatique
- [ ] ArgoCD se g√®re lui-m√™me via GitOps

---

## Phase 2 - D√©tail des Sprints

### Sprint 5 : Cilium L2 LoadBalancer (2-3h)
**Objectif** : LoadBalancer op√©rationnel via Cilium L2 Announcements

**Statut** : ‚úÖ Termin√©

**T√¢ches** :
1. Cr√©er `apps/metallb/base/` + `overlays/dev/`
2. D√©finir IPAddressPool (VLAN 208)
   - Pool assigned : .70-.79
   - Pool auto : .80-.89
3. Commit + push dev
4. Valider ArgoCD sync MetalLB

**Validation** :
```bash
kubectl get ipaddresspool -n metallb-system
kubectl get svc -n metallb-system
```

**Definition of Done** :
- [ ] MetalLB pods running
- [ ] IPAddressPool configur√©
- [ ] LoadBalancer service obtient IP du pool

---

### Sprint 6 : Traefik (3-4h)
**Objectif** : Ingress controller expos√©

**Statut** : [ ] En cours

**T√¢ches** :
1. Cr√©er `apps/traefik/base/` + `overlays/dev/`
2. Traefik LoadBalancer avec IP fixe (192.168.208.70)
3. D√©ployer app test (whoami) avec Ingress
4. Valider acc√®s HTTP externe

**Validation** :
```bash
kubectl get svc -n traefik traefik
# EXTERNAL-IP = 192.168.208.70

curl http://whoami.dev.local
# R√©sultat attendu : r√©ponse whoami
```

**Definition of Done** :
- [ ] Traefik accessible sur 192.168.208.70
- [ ] Ingress whoami fonctionnel
- [ ] HTTP routing valid√©

---

### Sprint 7 : cert-manager (Self-Signed Dev) (2-3h)
**Objectif** : TLS automatique en dev

**T√¢ches** :
1. Cr√©er `apps/cert-manager/base/`
2. ClusterIssuer `selfsigned` pour dev
3. Annoter Ingress whoami avec cert-manager
4. Valider certificat g√©n√©r√©

**Validation** :
```bash
kubectl get certificate -n default
# R√©sultat attendu : whoami-tls Ready

curl https://whoami.dev.local -k
# R√©sultat attendu : HTTPS avec self-signed cert
```

**Definition of Done** :
- [ ] cert-manager pods running
- [ ] ClusterIssuer selfsigned cr√©√©
- [ ] Certificat auto-g√©n√©r√© pour Ingress

---

### Sprint 8 : Synology CSI Driver (4-5h)
**Objectif** : Stockage iSCSI dynamique

**T√¢ches** :
1. Cr√©er Secret Synology (DSM credentials)
2. D√©ployer Synology CSI via ArgoCD
3. Cr√©er StorageClass `synelia-iscsi`
4. Tester PVC (MariaDB test)

**Validation** :
```bash
kubectl get sc synelia-iscsi
kubectl get pvc -n test
# R√©sultat attendu : PVC bound

kubectl exec -it mariadb-0 -n test -- df -h /var/lib/mysql
# R√©sultat attendu : iSCSI volume mont√©
```

**Definition of Done** :
- [ ] Synology CSI pods running
- [ ] StorageClass cr√©√©
- [ ] PVC dynamique fonctionnel
- [ ] Test DB avec donn√©es persistantes

---

### Sprint 9 : R√©plication Cluster Test (6-8h)
**Objectif** : Valider Terraform + Kustomize sur 2e cluster

**Statut** : [ ] En cours

**T√¢ches** :
1. Cr√©er `terraform/environments/test/`
2. Variables : carny, celesty, citrine (VLAN 209)
3. `terraform apply` ‚Üí cluster test complet
4. Cr√©er branche `test`
5. Cr√©er overlays test pour toutes les apps
6. Valider ArgoCD test sync depuis branche `test`

**Validation** :
```bash
kubectl --kubeconfig kubeconfig-test get nodes
# R√©sultat attendu : 3 nodes Ready (test)

argocd app list --kubeconfig kubeconfig-test
# R√©sultat attendu : toutes apps synced
```

**Definition of Done** :
- [ ] Cluster test op√©rationnel (3 nodes)
- [ ] Toutes apps Phase 2 d√©ploy√©es sur test
- [ ] Kustomize overlays fonctionnels
- [ ] GitOps test autonome

---

### Sprint 10 : Authelia (4-5h)
**Objectif** : SSO devant Traefik

**T√¢ches** :
1. Cr√©er `apps/authelia/base/` + overlays
2. Backend flatfile (users.yaml)
3. Middleware Traefik pour Authelia
4. Prot√©ger ArgoCD + whoami avec auth

**Validation** :
```bash
curl https://argocd.dev.local
# R√©sultat attendu : redirect vers Authelia login
```

**Definition of Done** :
- [ ] Authelia pods running
- [ ] Login page accessible
- [ ] Middleware Traefik configur√©
- [ ] Services prot√©g√©s par auth

---

### Sprint 11 : Monitoring (Prometheus/Grafana) (4-6h)
**Objectif** : Observabilit√© cluster

**T√¢ches** :
1. D√©ployer kube-prometheus-stack
2. Configurer ServiceMonitors (Cilium, Traefik, ArgoCD)
3. Dashboards Grafana
4. Alertes basiques (node down, pod crash)

**Validation** :
```bash
kubectl get prometheus -n monitoring
kubectl get grafana -n monitoring

# Acc√®s Grafana UI ‚Üí voir m√©triques cluster
```

**Definition of Done** :
- [ ] Prometheus scrape targets OK
- [ ] Grafana dashboards visibles
- [ ] Alertes configur√©es

---

## Phase 3 - Applications (Projet S√©par√©)

**Sprints futurs** :
- Sprint 12 : NFS Storage (PV statiques)
- Sprint 13 : Media apps (Radarr, Sonarr)
- Sprint 14 : Downloads (SABnzbd)
- Sprint 15 : Vaultwarden
- Sprint 16 : Home Assistant

---

## Timeline Estim√©e

| Phase      | Sprints       | Temps Estim√© | Status     |
|------------|---------------|--------------|------------|
| Phase 0    | Sprint 0      | 1-2h         | ‚úÖ En cours|
| Phase 1    | Sprints 1-4   | 12-17h       | ‚è≥ Pending |
| Phase 2    | Sprints 5-11  | 23-32h       | ‚è≥ Pending |
| Phase 3    | Sprints 12+   | TBD          | üìÖ Future  |

**Total Phase 1+2** : ~35-50h (r√©partis sur plusieurs semaines)

---

## D√©pendances entre Sprints

```
Sprint 1 (Terraform 1 node)
  ‚îî‚îÄ> Sprint 2 (Cilium)
      ‚îî‚îÄ> Sprint 3 (3 nodes)
          ‚îî‚îÄ> Sprint 4 (ArgoCD)
              ‚îú‚îÄ> Sprint 5 (MetalLB)
              ‚îÇ   ‚îî‚îÄ> Sprint 6 (Traefik)
              ‚îÇ       ‚îî‚îÄ> Sprint 7 (cert-manager)
              ‚îî‚îÄ> Sprint 8 (Synology CSI)
              ‚îî‚îÄ> Sprint 9 (Cluster Test)
                  ‚îî‚îÄ> Sprint 10 (Authelia)
                      ‚îî‚îÄ> Sprint 11 (Monitoring)
```

---

## Crit√®res de Succ√®s Globaux

### Phase 1 (Infrastructure as Code)
- [x] Cluster dev cr√©√© via `terraform apply`
- [ ] Cluster destructible et recr√©able en < 30min
- [ ] 3 control planes HA fonctionnels
- [ ] CNI (Cilium) op√©rationnel
- [x] ArgoCD auto-g√©r√©

### Phase 2 (GitOps Services)
- [ ] Tous les services g√©r√©s via Git
- [ ] `git push` = d√©ploiement automatique
- [ ] HTTPS fonctionnel (self-signed dev, Let's Encrypt prod)
- [ ] Storage dynamique (iSCSI)
- [ ] Authelia SSO prot√®ge tous les services
- [ ] Monitoring complet (Prometheus/Grafana)
- [ ] Cluster test r√©plique dev avec succ√®s (en cours)

---

## Risques & Mitigations

| Risque | Impact | Probabilit√© | Mitigation |
|--------|--------|-------------|------------|
| Complexit√© Terraform Talos | Bloque Phase 1 | Moyenne | Tests unitaires, validation progressive |
| R√©seau VLAN mal configur√© | Bloque acc√®s services | Faible | Documentation d√©taill√©e, tests connectivit√© |
| ArgoCD sync issues | Bloque GitOps | Faible | Validation dry-run, logs d√©taill√©s |
| Storage iSCSI performance | D√©grade apps | Faible | Tests bench, tuning NAS |
| Learning curve Cilium | Ralentit debugging | Moyenne | Documentation Hubble, Slack community |

---

## Next Steps

1. ‚úÖ Review documentation architecture (en cours)
2. ‚è≥ Cr√©er sprints/t√¢ches dans Archon (apr√®s validation)
3. ‚è≥ D√©marrer Sprint 1 (Terraform module)

---

**Version** : 1.0
**Date** : 2025-10-30
**Auteur** : Infrastructure Team
