# üìù POST-MORTEM : Incident de perte de donn√©es et reset Home Assistant (PROD)

**Date :** 06 F√©vrier 2026
**Statut :** R√âSOLU (Restauration √† J-6)
**S√©v√©rit√© :** Critique (S1) - Interruption de service et corruption de donn√©es.

---

## 1. üìù R√âSUM√â EX√âCUTIF
Lors d'une op√©ration de "Goldification" (durcissement de s√©curit√© et optimisation), l'application Home Assistant a subi un crash en boucle suivi d'une r√©initialisation de sa base d'utilisateurs et de ses registres d'entit√©s. L'incident a √©t√© aggrav√© par un effet de bord du syst√®me de synchronisation S3 qui a propag√© l'√©tat "vide" vers les sauvegardes distantes. La restauration a √©t√© effectu√©e avec succ√®s √† partir d'un backup local fourni par l'utilisateur.

---

## 2. ‚è≥ CHRONOLOGIE DES √âV√âNEMENTS
- **17:40** : Application du nouveau standard "Elite" sur Home Assistant (ajout de VPA, durcissement `securityContext`).
- **17:45** : **D√©clenchement du crash** : Le container refuse de d√©marrer (`Permission denied` sur `/config/.ha_run.lock`).
- **17:50** : Probl√®mes de montage iSCSI sur le cluster (timeouts). Kubernetes tente un `mke2fs` sur le volume, croyant qu'il est vierge.
- **18:00 - 19:30** : Tentatives de correction du `securityContext`. Le pod finit par monter le volume mais Home Assistant affiche l'√©cran d'onboarding.
- **20:00** : **D√©couverte de la corruption S3** : On s'aper√ßoit que le container `config-syncer` (rclone) a synchronis√© un dossier local vide vers Minio, effa√ßant ainsi le backup cloud.
- **21:30** : Analyse profonde du `/config` : Les fichiers YAML et la DB sont l√†, mais les fichiers d'authentification (`.storage/auth`) sont r√©initialis√©s.
- **22:15** : L'utilisateur fournit un backup manuel datant du 31/01 (J-6).
- **22:45** : Lancement de la proc√©dure de restauration manuelle via un pod `recovery`.
- **01:15** (J+1) : Fin de l'extraction, correction des permissions (`1000:1000`) et red√©marrage.
- **01:30** : **Confirmation de r√©tablissement** : Le portail de login est √† nouveau accessible.

---

## 3. üîç ANALYSE DES CAUSES RACINES (Root Causes)

1.  **Hardening inadapt√©** : L'application du standard `runAsNonRoot: true` sur une image non-native Kubernetes (`ghcr.io/home-assistant/home-assistant`) a bloqu√© les scripts d'initialisation (s6-overlay) qui n√©cessitent des privil√®ges root pour g√©rer les verrous et les permissions au d√©marrage.
2.  **Blind Sync (Le "Tueur de Backup")** : Le container de synchronisation `config-syncer` utilisait la commande `rclone sync`. Dans un √©tat de d√©faillance iSCSI o√π le dossier source para√Æt vide, `rclone sync` a fid√®lement reproduit cet √©tat vide sur la destination (Minio), supprimant les sauvegardes valides.
3.  **Absence de Snapshot LUN** : Le NAS Synology n'avait pas de politique de snapshot active pour ce LUN iSCSI, emp√™chant un rollback instantan√© au niveau bloc.

---

## 4. üõ°Ô∏è ACTIONS CORRECTIVES & PR√âVENTION

### Imm√©diat (Fait) :
*   **Restauration fonctionnelle** : Retour au backup J-6.
*   **Fix Permissions** : Ajout d'un init-container permanent `fix-perms` pour garantir que l'UID 1000 poss√®de toujours le volume, quel que soit le mode de d√©marrage du container principal.
*   **Rollback S√©curit√©** : Suppression des restrictions `securityContext` au niveau container pour cette application sp√©cifique.

### Recommandations √† court terme (√Ä faire) :
1.  **S√©curisation du Sync S3** : Remplacer `rclone sync` par `rclone copy` ou ajouter le flag `--max-delete 0` pour emp√™cher la suppression de fichiers sur le backup cloud en cas d'anomalie locale.
2.  **Politique de Snapshots** : Activer les snapshots toutes les 4h sur le LUN iSCSI via Synology Snapshot Replication.
3.  **Monitoring des Inits** : Ajouter des alertes Robusta sp√©cifiques si un container d'init reste en `PodInitializing` plus de 10 minutes.

---

## 5. üí° LE√áONS APPRISES
*   **Elite n'est pas Universel** : Le standard "Elite" (Rootless) ne peut pas √™tre appliqu√© par d√©faut sur des images "monolithiques" ou h√©rit√©es du monde Docker-compose sans une phase de test approfondie sur les points de montage syst√®me.
*   **L'importance du Backup "Froid"** : Sans ton fichier `.tar` mis de c√¥t√©, l'instance √©tait totalement perdue suite √† l'effacement auto du backup S3.
*   **iSCSI & K8s** : En cas de "Multi-Attach error", la solution la plus s√ªre est de scaler √† 0, attendre le timeout iSCSI (environ 2 min), puis relancer, plut√¥t que de tenter des suppressions forc√©es.

---

**Fin du rapport.**
